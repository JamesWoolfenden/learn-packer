{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Learn Packer By James Woolfenden What is Packer Packer is a cross platform and open source machine image creation tool, made by Hashicorp. Packer can make the images that launch our instances. When the term \" Infrastructure As Code \" is used, it is one of the foundational tools that are attributed to that term. Packer can be used to make Machine Images, be they Virtual Machine Server Instances or Containers. Hashicorps Packer is fully open source and the sourcecode is on Github here https://github.com/hashicorp/packer , and as you can see it is written in Golang. As well as examining the code you can see current issues/defects https://github.com/hashicorp/packer/issues and submit your own. All the examples can be found and copied from https://github.com/JamesWoolfenden/learn-packer/tree/master/examples The Packer term \"templates\" is used to refer to the files passed to the tool, are JSON files. base-ami.json below is a fully fledged AMI creastion example for AWS. { \"variables\": { \"aws_access_key\": \"{{env `AWS_ACCESS_KEY_ID`}}\", \"aws_secret_key\": \"{{env `AWS_SECRET_ACCESS_KEY`}}\", \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\", \"build_number\": \"{{env `BUILD_NUMBER`}}\", \"aws-region\": \"{{env `AWS_REGION`}}\" }, \"provisioners\": [ { \"type\": \"shell\", \"script\": \"provisioners/scripts/linux/redhat/install_ansible.sh\" }, { \"type\": \"shell\", \"script\": \"provisioners/scripts/linux/redhat/install_aws_ssm.sh\" }, { \"type\": \"ansible-local\", \"playbook_file\": \"provisioners/ansible/playbooks/cloudwatch-metrics.yml\", \"role_paths\": [ \"provisioners/ansible/roles/cloudwatch-metrics\" ] } ], \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\", \"token\": \"{{user `aws_session_token`}}\", \"region\": \"{{user `aws_region`}}\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"RHEL-7.6_HVM_GA-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"309956199498\" ], \"most_recent\": true }, \"instance_type\": \"{{ user `instance_type` }}\", \"ssh_username\": \"ec2-user\", \"ami_name\": \"RHEL-BASE-v{{user `build_number`}}-{{timestamp}}-AMI\", \"ami_description\": \"RHEL base AMI\", \"ami_virtualization_type\": \"hvm\", \"ami_users\": \"{{ user `ami_users` }}\", \"temporary_key_pair_name\": \"rhel-packer-{{timestamp}}\", \"vpc_id\": \"{{user `vpc_id`}}\", \"subnet_id\": \"{{user `subnet_id`}}\", \"associate_public_ip_address\": true, \"run_tags\": { \"Name\": \"rhel-base-packer\", \"Application\": \"base\" }, \"tags\": { \"OS_Version\": \"RedHat7\", \"Version\": \"{{user `build_number`}}\", \"Application\": \"Base Image\", \"Runner\": \"EC2\" } } ] } There is a lot going on there. That file has three of the four sections you'll find in Packer templates: variables builders Targets what you are making, you can have multiple builds that run in parallel. Provisioners Run after provisioners, these run in order on top of what you are building. Only a post-processor missing here. A common use of a post-processor would be to make a Vagrant Image from your builder, or to tag and push your Docker Images to their Repository. This is achieved by adding a new section to your template, only certain pst-processors work with each builder: { \"post-processors\": [\"Vagrant\"] } Packer is on a fairly fast update cycle, what may not have been supported in one release may get addressed and released. You can stay abreast of the changes via their changelog. $ packer Usage: packer [--version] [--help] <command> [<args>] Available commands are: build build image(s) from template fix fixes templates from old versions of packer inspect see components of a template validate check that a template is valid version Prints the Packer version $ packer version Packer v1.5.2 Current release of Packer Why Packer The rationale for using Packer is for it to help you create image an factory, a production line for Images, no matter what type to base your infrastructure on. Creating immutable images, with the only missing piece being environment specific configuration and keeping that to a minimum. Immutable Images are expected to be Stateless. Why not update the images or Configuration manage the images In place updates are riskier due to possible downtime, and applying fixes or updates to your current production Instance and Images could case an outage. Rollback is problematic with in-place updates. Basing the approach on AMI's gives you: repeatability quick boot times no reliance on externalities or third parties treat images as software artefacts with same CI process and versioning What are the alternatives to Packer Scripting it all yourself using: Ansible by itself Bash Powershell SSH But that's if you like maintaining a large home-grown script codebase. And more recently: AWS imagebuilder Note https://en.wikipedia.org/wiki/Software_release_life_cycle#RTM < https : // www . hashicorp . com / resources / what - is - mutable - vs - immutable - infrastructure > < https : // aws . amazon . com / about - aws / whats - new / 2019 / 12 / introducing - ec2 - image - builder > Made with Mkdocs, for full documentation visit mkdocs.org .","title":"Home"},{"location":"#learn-packer","text":"By James Woolfenden","title":"Learn Packer"},{"location":"#what-is-packer","text":"Packer is a cross platform and open source machine image creation tool, made by Hashicorp. Packer can make the images that launch our instances. When the term \" Infrastructure As Code \" is used, it is one of the foundational tools that are attributed to that term. Packer can be used to make Machine Images, be they Virtual Machine Server Instances or Containers. Hashicorps Packer is fully open source and the sourcecode is on Github here https://github.com/hashicorp/packer , and as you can see it is written in Golang. As well as examining the code you can see current issues/defects https://github.com/hashicorp/packer/issues and submit your own. All the examples can be found and copied from https://github.com/JamesWoolfenden/learn-packer/tree/master/examples The Packer term \"templates\" is used to refer to the files passed to the tool, are JSON files. base-ami.json below is a fully fledged AMI creastion example for AWS. { \"variables\": { \"aws_access_key\": \"{{env `AWS_ACCESS_KEY_ID`}}\", \"aws_secret_key\": \"{{env `AWS_SECRET_ACCESS_KEY`}}\", \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\", \"build_number\": \"{{env `BUILD_NUMBER`}}\", \"aws-region\": \"{{env `AWS_REGION`}}\" }, \"provisioners\": [ { \"type\": \"shell\", \"script\": \"provisioners/scripts/linux/redhat/install_ansible.sh\" }, { \"type\": \"shell\", \"script\": \"provisioners/scripts/linux/redhat/install_aws_ssm.sh\" }, { \"type\": \"ansible-local\", \"playbook_file\": \"provisioners/ansible/playbooks/cloudwatch-metrics.yml\", \"role_paths\": [ \"provisioners/ansible/roles/cloudwatch-metrics\" ] } ], \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\", \"token\": \"{{user `aws_session_token`}}\", \"region\": \"{{user `aws_region`}}\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"RHEL-7.6_HVM_GA-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"309956199498\" ], \"most_recent\": true }, \"instance_type\": \"{{ user `instance_type` }}\", \"ssh_username\": \"ec2-user\", \"ami_name\": \"RHEL-BASE-v{{user `build_number`}}-{{timestamp}}-AMI\", \"ami_description\": \"RHEL base AMI\", \"ami_virtualization_type\": \"hvm\", \"ami_users\": \"{{ user `ami_users` }}\", \"temporary_key_pair_name\": \"rhel-packer-{{timestamp}}\", \"vpc_id\": \"{{user `vpc_id`}}\", \"subnet_id\": \"{{user `subnet_id`}}\", \"associate_public_ip_address\": true, \"run_tags\": { \"Name\": \"rhel-base-packer\", \"Application\": \"base\" }, \"tags\": { \"OS_Version\": \"RedHat7\", \"Version\": \"{{user `build_number`}}\", \"Application\": \"Base Image\", \"Runner\": \"EC2\" } } ] } There is a lot going on there. That file has three of the four sections you'll find in Packer templates: variables builders Targets what you are making, you can have multiple builds that run in parallel. Provisioners Run after provisioners, these run in order on top of what you are building. Only a post-processor missing here. A common use of a post-processor would be to make a Vagrant Image from your builder, or to tag and push your Docker Images to their Repository. This is achieved by adding a new section to your template, only certain pst-processors work with each builder: { \"post-processors\": [\"Vagrant\"] } Packer is on a fairly fast update cycle, what may not have been supported in one release may get addressed and released. You can stay abreast of the changes via their changelog. $ packer Usage: packer [--version] [--help] <command> [<args>] Available commands are: build build image(s) from template fix fixes templates from old versions of packer inspect see components of a template validate check that a template is valid version Prints the Packer version $ packer version Packer v1.5.2 Current release of Packer","title":"What is Packer"},{"location":"#why-packer","text":"The rationale for using Packer is for it to help you create image an factory, a production line for Images, no matter what type to base your infrastructure on. Creating immutable images, with the only missing piece being environment specific configuration and keeping that to a minimum. Immutable Images are expected to be Stateless.","title":"Why Packer"},{"location":"#why-not-update-the-images-or-configuration-manage-the-images","text":"In place updates are riskier due to possible downtime, and applying fixes or updates to your current production Instance and Images could case an outage. Rollback is problematic with in-place updates. Basing the approach on AMI's gives you: repeatability quick boot times no reliance on externalities or third parties treat images as software artefacts with same CI process and versioning","title":"Why not update the images or Configuration manage the images"},{"location":"#what-are-the-alternatives-to-packer","text":"Scripting it all yourself using: Ansible by itself Bash Powershell SSH But that's if you like maintaining a large home-grown script codebase. And more recently: AWS imagebuilder Note https://en.wikipedia.org/wiki/Software_release_life_cycle#RTM < https : // www . hashicorp . com / resources / what - is - mutable - vs - immutable - infrastructure > < https : // aws . amazon . com / about - aws / whats - new / 2019 / 12 / introducing - ec2 - image - builder > Made with Mkdocs, for full documentation visit mkdocs.org .","title":"What are the alternatives to Packer"},{"location":"about/","text":"About Author: James Woolfenden LinkedIn Bio I'm currently working as a Principal for Slalom and based out of London. I have a bit of experience in the DevOps field, I have worked for a number of consultancies directly and indirectly. Why When I started in this field we hardly if ever thought about Servers. I was working in application development, we made applications and almost as an afterthought we made installers. When we were ready, we burnt the installers to CD or even DVD, and we were done and waited for QA. If our effort ever passed QA we made went \"GOLD MASTER\" or Release to Manufacture (RTM). Releases took months. In order to test our applications, we needed a known clean state, and to achieve this clean slate we used tools like Norton Ghost to reset our disks and our OS. I could then test the installer and applications again, and more importantly we could repeat. I started writing scripts to try an automate this process, and to ensure that the process was repeatable. Later on the tools changed and it was VMs against VMWare. With VMWare came snapshots and scripting the installation of components, .dot net frameworks and drivers. I wrote a lot of scripts. The scripts started getting complex and then we started making sure that our scripts were Idempotent. Idempotent is defined by the OED as \"Denoting an element of a set which is unchanged in value when multiplied or otherwise operated on by itself.\", in this context and layman terms, it's designed to be re-runnable. I wrote a lot of Idempotent scripts too (bash, Ant, MSBuild and PowerShell mostly). Then Puppet and Chef arrived, the First Generation of \"DevOps\" Tooling. This helped give a framework to work with - a DSL or a Domain Specific Language. We also has to install a lot of agents, and master control servers as well. We shared more and we wrote less, but Ruby now. It was getting easy to make servers and easier to make New Environments. I could make new environments reliably and I could also decommission them, only being limited by the infrastructure made available. We started worrying about people changing our Servers (Ok Developers). How do we Manage the Configuration and get to a know and recorded state, to fix the \"Drift\". The difference from the desired state. All these Puppet and Chef scripts looked to manage Drift, the tools would then try to eliminate Drift and bring of environments back to the \"Desired State\". We'll we'd try to that is, as it's only as Ruby code you wrote and the packages they ran, how can you know all the cases/paths that describe how your state could deviate? Wouldn't it be better if I didn't have to? And besides some of these CM script suites took a really long time to run, hours. If you wanted to add one new package to a server or build agent. Agents everywhere. Running them in Prod was just scary. Then the Cloud. This gave a common API to make our infrastructure against, limits? Just cost now. Now we were using Ansible, all those agents were gone, but it still took quite some time to make instances. Wouldn't it be better to make these in advance? That would be quicker? What about just making new servers quickly from AMIs instead and just the Environmental configuration at Launch? When Ansible arrived we no longer had to add Agents/services but we still had to wait while Ansible built or checked the images. You could use Ansible to make AMIs directly but Packer is designed to solve just this problem. Packer with scripts is great, Packer with Ansible is even easier. What's even easier is when you find well explained samples.","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#author-james-woolfenden","text":"LinkedIn","title":"Author: James Woolfenden"},{"location":"about/#bio","text":"I'm currently working as a Principal for Slalom and based out of London. I have a bit of experience in the DevOps field, I have worked for a number of consultancies directly and indirectly.","title":"Bio"},{"location":"about/#why","text":"When I started in this field we hardly if ever thought about Servers. I was working in application development, we made applications and almost as an afterthought we made installers. When we were ready, we burnt the installers to CD or even DVD, and we were done and waited for QA. If our effort ever passed QA we made went \"GOLD MASTER\" or Release to Manufacture (RTM). Releases took months. In order to test our applications, we needed a known clean state, and to achieve this clean slate we used tools like Norton Ghost to reset our disks and our OS. I could then test the installer and applications again, and more importantly we could repeat. I started writing scripts to try an automate this process, and to ensure that the process was repeatable. Later on the tools changed and it was VMs against VMWare. With VMWare came snapshots and scripting the installation of components, .dot net frameworks and drivers. I wrote a lot of scripts. The scripts started getting complex and then we started making sure that our scripts were Idempotent. Idempotent is defined by the OED as \"Denoting an element of a set which is unchanged in value when multiplied or otherwise operated on by itself.\", in this context and layman terms, it's designed to be re-runnable. I wrote a lot of Idempotent scripts too (bash, Ant, MSBuild and PowerShell mostly). Then Puppet and Chef arrived, the First Generation of \"DevOps\" Tooling. This helped give a framework to work with - a DSL or a Domain Specific Language. We also has to install a lot of agents, and master control servers as well. We shared more and we wrote less, but Ruby now. It was getting easy to make servers and easier to make New Environments. I could make new environments reliably and I could also decommission them, only being limited by the infrastructure made available. We started worrying about people changing our Servers (Ok Developers). How do we Manage the Configuration and get to a know and recorded state, to fix the \"Drift\". The difference from the desired state. All these Puppet and Chef scripts looked to manage Drift, the tools would then try to eliminate Drift and bring of environments back to the \"Desired State\". We'll we'd try to that is, as it's only as Ruby code you wrote and the packages they ran, how can you know all the cases/paths that describe how your state could deviate? Wouldn't it be better if I didn't have to? And besides some of these CM script suites took a really long time to run, hours. If you wanted to add one new package to a server or build agent. Agents everywhere. Running them in Prod was just scary. Then the Cloud. This gave a common API to make our infrastructure against, limits? Just cost now. Now we were using Ansible, all those agents were gone, but it still took quite some time to make instances. Wouldn't it be better to make these in advance? That would be quicker? What about just making new servers quickly from AMIs instead and just the Environmental configuration at Launch? When Ansible arrived we no longer had to add Agents/services but we still had to wait while Ansible built or checked the images. You could use Ansible to make AMIs directly but Packer is designed to solve just this problem. Packer with scripts is great, Packer with Ansible is even easier. What's even easier is when you find well explained samples.","title":"Why"},{"location":"amazon-ebs/","text":"Amazon EBS TODO Specify a bastion TODO Tagging TODO Root Volume encryption TODO Via public route Via private route","title":"Amazon EBS"},{"location":"amazon-ebs/#amazon-ebs","text":"TODO","title":"Amazon EBS"},{"location":"amazon-ebs/#specify-a-bastion","text":"TODO","title":"Specify a bastion"},{"location":"amazon-ebs/#tagging","text":"TODO","title":"Tagging"},{"location":"amazon-ebs/#root-volume-encryption","text":"TODO","title":"Root Volume encryption"},{"location":"amazon-ebs/#via-public-route","text":"","title":"Via public route"},{"location":"amazon-ebs/#via-private-route","text":"","title":"Via private route"},{"location":"ansible-docker/","text":"Packer-Ansible-Docker As of writing there\u2019s still no support for running Ansible on Windows [except via WSL], so to run these examples you will have to have a Mac Or Linux. Pre-requisites Ansible needs to be installed. On Ubuntu that's: sudo apt update sudo apt install software-properties-common sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible For other platforms refer to or see the Ansible site itself https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html Ansible Galaxy is pre-packaged with Ansible. How to get an Ansible role Call Ansible Galaxy to download a role, in this case, a role to install Terraform: $ ansible-galaxy install --roles-path . migibert.terraform - downloading role 'terraform' , owned by migibert - downloading role from https://github.com/migibert/terraform-role/archive/1.3.tar.gz - extracting migibert.terraform to /c/code/book/Image-Creation-using-Packer/packer-ansible-docker/migibert.terraform - migibert.terraform ( 1 .3 ) was installed successfully You now have a folder migibert.terraform . You can build these up into playbooks and provisioners, I made many of these to make some Confluent/Kafka Images here: https://github.com/JamesWoolfenden/packer-by-example/blob/master/packfiles/redhat/confluent-connect.json \"provisioners\" : [ { \"type\" : \"ansible-local\" , \"playbook_file\" : \"provisioners/ansible/playbooks/confluent.yml\" , \"role_paths\" : [ \"provisioners/ansible/roles/openjdk\" , \"provisioners/ansible/roles/confluent.common\" ] }, where confluent.yml is: --- - hosts: localhost become: yes become_user: root roles: - openjdk - confluent.common","title":"Ansible-Docker"},{"location":"ansible-docker/#packer-ansible-docker","text":"As of writing there\u2019s still no support for running Ansible on Windows [except via WSL], so to run these examples you will have to have a Mac Or Linux.","title":"Packer-Ansible-Docker"},{"location":"ansible-docker/#pre-requisites","text":"Ansible needs to be installed. On Ubuntu that's: sudo apt update sudo apt install software-properties-common sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible For other platforms refer to or see the Ansible site itself https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html Ansible Galaxy is pre-packaged with Ansible.","title":"Pre-requisites"},{"location":"ansible-docker/#how-to-get-an-ansible-role","text":"Call Ansible Galaxy to download a role, in this case, a role to install Terraform: $ ansible-galaxy install --roles-path . migibert.terraform - downloading role 'terraform' , owned by migibert - downloading role from https://github.com/migibert/terraform-role/archive/1.3.tar.gz - extracting migibert.terraform to /c/code/book/Image-Creation-using-Packer/packer-ansible-docker/migibert.terraform - migibert.terraform ( 1 .3 ) was installed successfully You now have a folder migibert.terraform . You can build these up into playbooks and provisioners, I made many of these to make some Confluent/Kafka Images here: https://github.com/JamesWoolfenden/packer-by-example/blob/master/packfiles/redhat/confluent-connect.json \"provisioners\" : [ { \"type\" : \"ansible-local\" , \"playbook_file\" : \"provisioners/ansible/playbooks/confluent.yml\" , \"role_paths\" : [ \"provisioners/ansible/roles/openjdk\" , \"provisioners/ansible/roles/confluent.common\" ] }, where confluent.yml is: --- - hosts: localhost become: yes become_user: root roles: - openjdk - confluent.common","title":"How to get an Ansible role"},{"location":"docker-hcl2/","text":"Packer-Docker HCL2 Using Packer to build Docker might seem a bit strange at first. Aren't Dockerfiles a standard for building Docker containers? I'd be thinking this'll never fly with my JS developers? But It can make sense. To build with a Dockerfile, and to save to it to a registry requires 2 steps, first to build with a CI tool like Gitlab/Circle and requires a Dockerfile and a process to manage logging into the registry and pushing to it. In Packer this entire process can be captured in one file, encapsulating all that's required, and as a bonus the whole process is runnable locally. Which make for quick debug cycle times. Using Packer also allows the use and re-use of scripts and to use Ansible role and playbooks as used in other platform builds. These HCL2 based examples target folders not individual files. A very basic Packer Docker template Starting really simple with examples-hcl/packer-docker-ubuntu , this contains a source docker.base1604.pkr.hcl From examples\\packer-docker-ubuntu at https://github.com/jamesWoolfenden/learn-packer/ . source \"docker\" \"base1604\" { image= \"ubuntu\" export_path= \"export_image.tar\" } and build file build.base1604.pkr.hcl build { sources=[ \"source.docker.base1604\" ] } Building this template with.. $ packer build .\\packer-docker-ubuntu\\ docker: output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: ubuntu docker: Using default tag: latest docker: latest: Pulling from library/ubuntu docker: Digest: sha256:04d48df82c938587820d7b6006f5071dbbffceb7ca01d2814f81857c631d44df docker: Status: Image is up to date for ubuntu:latest docker: docker.io/library/ubuntu:latest ==> docker: Starting docker container... docker: Run command: docker run -v C:\\Users\\james.woolfenden\\packer.d\\tmp766628703:/packer-files -d -i -t --entrypoint=/bin/sh -- ubuntu docker: Container ID: dd47b23d66f207284f54d6cd9803aa8a219842909d14b4fe037336539df36cde ==> docker: Using docker communicator to connect: 172.17.0.2 ==> docker: Exporting the container ==> docker: Killing the container: dd47b23d66f207284f54d6cd9803aa8a219842909d14b4fe037336539df36cde Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Exported Docker file: export_image.tar OK so that's Not that useful yet. Yet. Tagging with a post-processor So far you have seen demonstrated that a build of a container with Packer, but it's not good for much yet. Next is to start plugging in other components. First up is a post-processor, used to tag the container. Add this section to build.base1604.pkr.hcl after the sources [examples\\packer-docker-post]. post-processor \"docker-tag\" { repository=\"james\" tag= [\"0.1\"] } and change docker.base1604.pkr.hcl to: source \"docker\" \"base1604\" { image= \"ubuntu\" commit= true } Still not exactly Rocket Science yet, is it. Building into an AWS registry TODO When this runs in Packer... `` With Shell provisioner ## Provisioners Push to ECR !!! Note The Cons - Packer build only works with docker images that already have SSH on them. - Shell Provisioners - Bash scripts only run if you have Bash installed in your container. - Packer Docker builds can't build from 'scratch'. - Packer Provisioners use SSH, images like a Base Alpine will fail, so you can only base your containers that have SSH installed. - No Layer caching.","title":"docker-hcl"},{"location":"docker-hcl2/#packer-docker-hcl2","text":"Using Packer to build Docker might seem a bit strange at first. Aren't Dockerfiles a standard for building Docker containers? I'd be thinking this'll never fly with my JS developers? But It can make sense. To build with a Dockerfile, and to save to it to a registry requires 2 steps, first to build with a CI tool like Gitlab/Circle and requires a Dockerfile and a process to manage logging into the registry and pushing to it. In Packer this entire process can be captured in one file, encapsulating all that's required, and as a bonus the whole process is runnable locally. Which make for quick debug cycle times. Using Packer also allows the use and re-use of scripts and to use Ansible role and playbooks as used in other platform builds. These HCL2 based examples target folders not individual files.","title":"Packer-Docker HCL2"},{"location":"docker-hcl2/#a-very-basic-packer-docker-template","text":"Starting really simple with examples-hcl/packer-docker-ubuntu , this contains a source docker.base1604.pkr.hcl From examples\\packer-docker-ubuntu at https://github.com/jamesWoolfenden/learn-packer/ . source \"docker\" \"base1604\" { image= \"ubuntu\" export_path= \"export_image.tar\" } and build file build.base1604.pkr.hcl build { sources=[ \"source.docker.base1604\" ] } Building this template with.. $ packer build .\\packer-docker-ubuntu\\ docker: output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: ubuntu docker: Using default tag: latest docker: latest: Pulling from library/ubuntu docker: Digest: sha256:04d48df82c938587820d7b6006f5071dbbffceb7ca01d2814f81857c631d44df docker: Status: Image is up to date for ubuntu:latest docker: docker.io/library/ubuntu:latest ==> docker: Starting docker container... docker: Run command: docker run -v C:\\Users\\james.woolfenden\\packer.d\\tmp766628703:/packer-files -d -i -t --entrypoint=/bin/sh -- ubuntu docker: Container ID: dd47b23d66f207284f54d6cd9803aa8a219842909d14b4fe037336539df36cde ==> docker: Using docker communicator to connect: 172.17.0.2 ==> docker: Exporting the container ==> docker: Killing the container: dd47b23d66f207284f54d6cd9803aa8a219842909d14b4fe037336539df36cde Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Exported Docker file: export_image.tar OK so that's Not that useful yet. Yet.","title":"A very basic Packer Docker template"},{"location":"docker-hcl2/#tagging-with-a-post-processor","text":"So far you have seen demonstrated that a build of a container with Packer, but it's not good for much yet. Next is to start plugging in other components. First up is a post-processor, used to tag the container. Add this section to build.base1604.pkr.hcl after the sources [examples\\packer-docker-post]. post-processor \"docker-tag\" { repository=\"james\" tag= [\"0.1\"] } and change docker.base1604.pkr.hcl to: source \"docker\" \"base1604\" { image= \"ubuntu\" commit= true } Still not exactly Rocket Science yet, is it.","title":"Tagging with a post-processor"},{"location":"docker-hcl2/#building-into-an-aws-registry","text":"TODO When this runs in Packer... `` With Shell provisioner ## Provisioners Push to ECR !!! Note The Cons - Packer build only works with docker images that already have SSH on them. - Shell Provisioners - Bash scripts only run if you have Bash installed in your container. - Packer Docker builds can't build from 'scratch'. - Packer Provisioners use SSH, images like a Base Alpine will fail, so you can only base your containers that have SSH installed. - No Layer caching.","title":"Building into an AWS registry"},{"location":"docker/","text":"Packer-Docker Using Packer to build Docker might seem a bit strange at first. Aren't Dockerfiles a standard for building Docker containers? I'd be thinking this'll never fly with my JS developers? But It can make sense. To build with a Dockerfile, and to save to it to a registry requires 2 steps, first to build with a CI tool like Gitlab/Circle and requires a Dockerfile and a process to manage logging into the registry and pushing to it. In Packer this entire process can be captured in one file, encapsulating all that's required, and as a bonus the whole process is runnable locally. Which make for quick debug cycle times. Using Packer also allows the use and re-use of scripts and to use Ansible role and playbooks as used in other platform builds. A very basic Packer Docker template Starting really simple with packer-docker-ubuntu.json . { \"_comment\" : \"Docker\" , \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"ubuntu\" , \"export_path\" : \"image.tar\" } ] } Building this template with.. $ packer build packer-docker-ubuntu.json docker output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: ubuntu docker: Using default tag: latest docker: latest: Pulling from library/ubuntu docker: 898c46f3b1a1: Pulling fs layer docker: 63366dfa0a50: Pulling fs layer docker: 041d4cd74a92: Pulling fs layer docker: 6e1bee0f8701: Pulling fs layer docker: 6e1bee0f8701: Waiting docker: 041d4cd74a92: Verifying Checksum docker: 041d4cd74a92: Download complete docker: 63366dfa0a50: Verifying Checksum docker: 63366dfa0a50: Download complete docker: 6e1bee0f8701: Download complete docker: 898c46f3b1a1: Verifying Checksum docker: 898c46f3b1a1: Download complete docker: 898c46f3b1a1: Pull complete docker: 63366dfa0a50: Pull complete docker: 041d4cd74a92: Pull complete docker: 6e1bee0f8701: Pull complete docker: Digest: sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8 docker: Status: Downloaded newer image for ubuntu:latest ==> docker: Starting docker container... docker: Run command: docker run -v /home/jim/.packer.d/tmp:/packer-files -d -i -t ubuntu /bin/bash docker: Container ID: 2dc59e83c81eef5feb0bfa5370a1616bdae5effa93b01ca6378716815582425d ==> docker: Using docker communicator to connect: 172.17.0.2 ==> docker: Exporting the container ==> docker: Killing the container: 2dc59e83c81eef5feb0bfa5370a1616bdae5effa93b01ca6378716815582425d Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Exported Docker file: image.tar OK so that's Not that useful yet. Yet. Tagging with a post-processor So far you have seen demonstrated that a build of a container with Packer, but it's not good for much yet. Next is to start plugging in other components. First up is a post-processor, used to tag the container. Enter the example packer-docker-tag-empty.json { \"variables\" : { \"tag\" : \"{{env `BUILD_NUMBER`}}\" }, \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"ubuntu\" , \"commit\" : true } ], \"post-processors\" : [ { \"type\" : \"docker-tag\" , \"tag\" : \"{{user `tag`}}\" } ] } Still not exactly Rocket Science yet, is it. Trying it out. $ packer build -var tag=2 packer-docker-tag-empty.json docker output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: ubuntu docker: Using default tag: latest docker: latest: Pulling from library/ubuntu docker: Digest: sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8 docker: Status: Image is up to date for ubuntu:latest ==> docker: Starting docker container... docker: Run command: docker run -v /home/jim/.packer.d/tmp:/packer-files -d -i -t ubuntu /bin/bash docker: Container ID: fe20e550df8e38e99a4a87ffe1836c646eecce7cf31db5da67f55d80c6ba54f8 ==> docker: Using docker communicator to connect: 172.17.0.2 ==> docker: Committing the container docker: Image ID: sha256:bd89cee10dc09fd28dd5cf1d00fc62fd659db014ef96b2d62a74eccf88c4b70d ==> docker: Killing the container: fe20e550df8e38e99a4a87ffe1836c646eecce7cf31db5da67f55d80c6ba54f8 ==> docker: Running post-processor: docker-tag docker (docker-tag): Tagging image: sha256:bd89cee10dc09fd28dd5cf1d00fc62fd659db014ef96b2d62a74eccf88c4b70d docker (docker-tag): Repository: dave:2 Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Imported Docker image: sha256:bd89cee10dc09fd28dd5cf1d00fc62fd659db014ef96b2d62a74eccf88c4b70d --> docker: Imported Docker image: dave:2 So now we have a tagged Dave:2 container. Contain your excitement. Ok let's move on. Building into an AWS registry This next example will do everything, ok the main stuff for a container, build, tag and push to ECR. Running packer build .\\packer-docker-aws-ecr.json { \"_comment\" : \"Docker AWS ECR\" , \"variables\" : { \"tag\" : \"{{env `BUILD_NUMBER`}}\" , \"repository\" : \"123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container\" , \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"login_server\" : \"https://123456789012.dkr.ecr.eu-west-1.amazonaws.com/\" }, \"_comment\" : \"SSH needs to be on container for shell provisioner\" , \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"hashicorp/terraform\" , \"commit\" : true } ], \"post-processors\" : [ [ { \"type\" : \"docker-tag\" , \"tag\" : \"{{user `tag`}}\" , \"repository\" : \"{{user `repository`}}\" }, { \"type\" : \"docker-push\" , \"ecr_login\" : true , \"aws_access_key\" : \"{{user `aws_access_key`}}\" , \"aws_secret_key\" : \"{{user `aws_secret_key`}}\" , \"login_server\" : \"{{user `login_server`}}\" } ] ] } When this runs in Packer... docker output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: hashicorp/terraform docker: Using default tag: latest docker: latest: Pulling from hashicorp/terraform docker: Digest: sha256:330bef7401e02e757e6fa2de69f398fd29fcbfafe2a3b9e8f150486fbcd7915b docker: Status: Image is up to date for hashicorp/terraform:latest ==> docker: Starting docker container... docker: Run command: docker run -v /c/Users/james.woolfenden/packer.d/tmp:/packer-files -d -i -t hashicorp/terraform /bin/bash docker: Container ID: fa8f17d078f1978cda0a7b0c9352e7afffc8a2cb57db425faff3cc8f9816cf5d ==> docker: Using docker communicator to connect: ==> docker: Committing the container docker: Image ID: sha256:759f4b14fe9d477a7a6492ff9c1becd77e3d8b1e01a89f07275c8d72f3e7907d ==> docker: Killing the container: fa8f17d078f1978cda0a7b0c9352e7afffc8a2cb57db425faff3cc8f9816cf5d ==> docker: Running post-processor: docker-tag docker (docker-tag): Tagging image: sha256:759f4b14fe9d477a7a6492ff9c1becd77e3d8b1e01a89f07275c8d72f3e7907d docker (docker-tag): Repository: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 ==> docker: Running post-processor: docker-push docker (docker-push): Fetching ECR credentials... docker (docker-push): Logging in... docker (docker-push): Login Succeeded docker (docker-push): Pushing: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 docker (docker-push): The push refers to repository [123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container] docker (docker-push): 6ad0db50b392: Preparing docker (docker-push): 1eeb4487de94: Preparing docker (docker-push): 9094b98cdb25: Preparing docker (docker-push): 503e53e365f3: Preparing docker (docker-push): 9094b98cdb25: Layer already exists docker (docker-push): 1eeb4487de94: Layer already exists docker (docker-push): 503e53e365f3: Layer already exists docker (docker-push): 6ad0db50b392: Pushed docker (docker-push): 5: digest: sha256:82de02fbaab0a2c35d033cae5278c59e1d8e65ee2bb4cb5a57b803536b305a3c size: 1155 docker (docker-push): Logging out... docker (docker-push): Removing login credentials for 123456789012.dkr.ecr.eu-west-1.amazonaws.com Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Imported Docker image: sha256:759f4b14fe9d477a7a6492ff9c1becd77e3d8b1e01a89f07275c8d72f3e7907d --> docker: Imported Docker image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 --> docker: Imported Docker image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 While that's great, we added nothing to the container so it doesn't prove that much yet. Sure it connected to ECR and pushed the container, but we added nothing to it, for that, we need to add some Provisioners. Packer builds containers will only work with Docker images that already have SSH on them. Reading this will save you a lot of time. I've changed the base image from hashicorp/terraform to ebiwd/alpine-ssh , and added terraform to its package list. This the shell script it runs on the Provisioner install_awscli.sh . #!/bin/sh apk update apk add --update nodejs-current nodejs-npm terraform apk add --update python python-dev py-pip build-base pip install --upgrade pip pip install awscli Provisioners This next example combines the lot, it's imaginatively called packer-docker-aws-ecr-shell.json { \"_comment\" : \"Docker AWS ECR\" , \"variables\" : { \"tag\" : \"{{env `BUILD_NUMBER`}}\" , \"repository\" : \"123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container\" , \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"login_server\" : \"https://123456789012.dkr.ecr.eu-west-1.amazonaws.com/\" }, \"_comment\" : \"SSH needs to be on container for shell provisioner\" , \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"ebiwd/alpine-ssh\" , \"commit\" : true } ], \"_comment\" : \"probs bash needs to be there\" , \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"apk add bash\" ] }, { \"type\" : \"shell\" , \"script\" : \"install-awscli.sh\" } ], \"post-processors\" : [ [ { \"type\" : \"docker-tag\" , \"tag\" : \"{{user `tag`}}\" , \"repository\" : \"{{user `repository`}}\" }, { \"type\" : \"docker-push\" , \"ecr_login\" : true , \"aws_access_key\" : \"{{user `aws_access_key`}}\" , \"aws_secret_key\" : \"{{user `aws_secret_key`}}\" , \"login_server\" : \"{{user `login_server`}}\" } ] ] } Let's built it: $ packer build . \\p acker-docker-aws-ecr.json docker output will be in this color. == > docker: Creating a temporary directory for sharing data... == > docker: Pulling Docker image: ebiwd/alpine-ssh docker: Using default tag: latest docker: latest: Pulling from ebiwd/alpine-ssh docker: 59265c40e257: Pulling fs layer docker: a621cd180b0b: Pulling fs layer docker: 1388eaedce13: Pulling fs layer docker: 75cc2d2e3f13: Pulling fs layer docker: b4ac607039c6: Pulling fs layer docker: 75cc2d2e3f13: Waiting docker: b4ac607039c6: Waiting docker: 59265c40e257: Verifying Checksum docker: 59265c40e257: Download complete docker: 59265c40e257: Pull complete docker: 75cc2d2e3f13: Verifying Checksum docker: 75cc2d2e3f13: Download complete docker: b4ac607039c6: Download complete docker: a621cd180b0b: Verifying Checksum docker: a621cd180b0b: Download complete docker: a621cd180b0b: Pull complete docker: 1388eaedce13: Verifying Checksum docker: 1388eaedce13: Download complete docker: 1388eaedce13: Pull complete docker: 75cc2d2e3f13: Pull complete docker: b4ac607039c6: Pull complete docker: Digest: sha256:6d994480f495fc7ddc16d126e4c41f9fc0153363b6f9e686f50d47039078615c docker: Status: Downloaded newer image for ebiwd/alpine-ssh:latest == > docker: Starting docker container... docker: Run command: docker run -v /c/Users/SOMEUSER/packer.d/tmp:/packer-files -d -i -t ebiwd/alpine-ssh /bin/bash docker: Container ID: 5fbda610d9117b5a2c20f5da6e77b4cb230114c44fbf79b168a30e32e3bc26f2 == > docker: Using docker communicator to connect: 172 .17.0.2 == > docker: Provisioning with shell script: C: \\U sers \\S OMEUSER \\A ppData \\L ocal \\T emp \\p acker-shell350255759 docker: OK: 68 MiB in 39 packages docker: WARNING: Ignoring APKINDEX.84815163.tar.gz: No such file or directory docker: WARNING: Ignoring APKINDEX.24d64ab1.tar.gz: No such file or directory == > docker: Provisioning with shell script: install-awscli.sh docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz docker: v3.6.5-5-geec223036a [ http://dl-cdn.alpinelinux.org/alpine/v3.6/main ] docker: v3.6.5-4-g1bf6e4dfc6 [ http://dl-cdn.alpinelinux.org/alpine/v3.6/community ] docker: OK: 8449 distinct packages available docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz docker: ( 1 /8 ) Installing libcrypto1.0 ( 1 .0.2r-r0 ) docker: ( 2 /8 ) Installing libgcc ( 6 .3.0-r4 ) docker: ( 3 /8 ) Installing http-parser ( 2 .7.1-r1 ) docker: ( 4 /8 ) Installing libssl1.0 ( 1 .0.2r-r0 ) docker: ( 5 /8 ) Installing libstdc++ ( 6 .3.0-r4 ) docker: ( 6 /8 ) Installing libuv ( 1 .11.0-r1 ) docker: ( 7 /8 ) Installing nodejs-current ( 7 .10.1-r1 ) docker: ( 8 /8 ) Installing nodejs-npm ( 6 .10.3-r2 ) docker: Executing busybox-1.26.2-r11.trigger docker: OK: 110 MiB in 47 packages docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz docker: ( 1 /19 ) Upgrading musl ( 1 .1.16-r14 -> 1 .1.16-r15 ) docker: ( 2 /19 ) Installing binutils-libs ( 2 .30-r1 ) docker: ( 3 /19 ) Installing binutils ( 2 .30-r1 ) docker: ( 4 /19 ) Installing gmp ( 6 .1.2-r0 ) docker: ( 5 /19 ) Installing isl ( 0 .17.1-r0 ) docker: ( 6 /19 ) Installing libgomp ( 6 .3.0-r4 ) docker: ( 7 /19 ) Installing libatomic ( 6 .3.0-r4 ) docker: ( 8 /19 ) Installing pkgconf ( 1 .3.7-r0 ) docker: ( 9 /19 ) Installing mpfr3 ( 3 .1.5-r0 ) docker: ( 10 /19 ) Installing mpc1 ( 1 .0.3-r0 ) docker: ( 11 /19 ) Installing gcc ( 6 .3.0-r4 ) docker: ( 12 /19 ) Installing musl-dev ( 1 .1.16-r15 ) docker: ( 13 /19 ) Installing libc-dev ( 0 .7.1-r0 ) docker: ( 14 /19 ) Installing g++ ( 6 .3.0-r4 ) docker: ( 15 /19 ) Installing make ( 4 .2.1-r0 ) docker: ( 16 /19 ) Installing fortify-headers ( 0 .8-r0 ) docker: ( 17 /19 ) Installing build-base ( 0 .5-r0 ) docker: ( 18 /19 ) Upgrading musl-utils ( 1 .1.16-r14 -> 1 .1.16-r15 ) docker: ( 19 /19 ) Installing python2-dev ( 2 .7.15-r0 ) docker: Executing busybox-1.26.2-r11.trigger docker: OK: 274 MiB in 64 packages docker: Collecting pip docker: Downloading https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl ( 1 .4MB ) docker: Installing collected packages: pip docker: Found existing installation: pip 9 .0.1 docker: Uninstalling pip-9.0.1: docker: Successfully uninstalled pip-9.0.1 docker: Successfully installed pip-19.0.3 docker: DEPRECATION: Python 2 .7 will reach the end of its life on January 1st, 2020 . Please upgrade your Python as Python 2 .7 won 't be maintained after that date. A future version of pip will drop support for Python 2.7. docker: Requirement already satisfied: awscli in /usr/lib/python2.7/site-packages (1.16.106) docker: Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/lib/python2.7/site-packages (from awscli) (0.2.0) docker: Requirement already satisfied: docutils>=0.10 in /usr/lib/python2.7/site-packages (from awscli) (0.14) docker: Requirement already satisfied: botocore==1.12.96 in /usr/lib/python2.7/site-packages (from awscli) (1.12.96) docker: Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/lib/python2.7/site-packages (from awscli) (3.13) docker: Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /usr/lib/python2.7/site-packages (from awscli) (0.3.9) docker: Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/lib/python2.7/site-packages (from awscli) (3.4.2) docker: Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/lib/python2.7/site-packages (from s3transfer<0.3.0,>=0.2.0->awscli) (3.2.0) docker: Requirement already satisfied: urllib3<1.25,>=1.20; python_version == \"2.7\" in /usr/lib/python2.7/site-packages (from botocore==1.12.96->awscli) (1.24.1) docker: Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/lib/python2.7/site-packages (from botocore==1.12.96->awscli) (2.8.0) docker: Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/lib/python2.7/site-packages (from botocore==1.12.96->awscli) (0.9.3) docker: Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python2.7/site-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.5) docker: Requirement already satisfied: six>=1.5 in /usr/lib/python2.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore==1.12.96->awscli) (1.12.0) ==> docker: Committing the container docker: Image ID: sha256:8af22b4e91c0e6ebc53e309ac3d803c989e7007c9e6c3f57dadeda0d059006d6 ==> docker: Killing the container: 5fbda610d9117b5a2c20f5da6e77b4cb230114c44fbf79b168a30e32e3bc26f2 ==> docker: Running post-processor: docker-tag docker (docker-tag): Tagging image: sha256:8af22b4e91c0e6ebc53e309ac3d803c989e7007c9e6c3f57dadeda0d059006d6 docker (docker-tag): Repository: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 ==> docker: Running post-processor: docker-push docker (docker-push): Fetching ECR credentials... docker (docker-push): Logging in... docker (docker-push): Login Succeeded docker (docker-push): Pushing: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 docker (docker-push): The push refers to repository [123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container] docker (docker-push): 17073917cb42: Preparing docker (docker-push): 96287b9167c6: Preparing docker (docker-push): 8619e10b942a: Preparing docker (docker-push): e74e4c2ec07f: Preparing docker (docker-push): 780b2a6e852d: Preparing docker (docker-push): e79522dce35e: Preparing docker (docker-push): e79522dce35e: Waiting docker (docker-push): 96287b9167c6: Pushed docker (docker-push): e79522dce35e: Pushed docker (docker-push): 8619e10b942a: Pushed docker (docker-push): 780b2a6e852d: Pushed docker (docker-push): e74e4c2ec07f: Pushed docker (docker-push): 17073917cb42: Pushed docker (docker-push): 5: digest: sha256:5407f913fb6493f6d67079b562bfc570775c4a649eaf1ee58c6151abfea0b139 size: 1582 docker (docker-push): Logging out... docker (docker-push): Removing login credentials for 123456789012.dkr.ecr.eu-west-1.amazonaws.com Build ' docker ' finished. == > Builds finished. The artifacts of successful builds are: --> docker: Imported Docker image: sha256:8af22b4e91c0e6ebc53e309ac3d803c989e7007c9e6c3f57dadeda0d059006d6 --> docker: Imported Docker image: 123456789012 .dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 --> docker: Imported Docker image: 123456789012 .dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 So that's an example of nearly every key component to build your Dockerfile. All that's required is a tool to host it in. Note Packer build only works with docker images that already have SSH on them. Shell Provisioners - Bash scripts only run if you have Bash installed in your container. Packer Docker builds can't build from 'scratch'. Packer Provisioners use SSH, images like a Base Alpine will fail, so you can only base your containers that have SSH installed. No Layer caching.","title":"docker"},{"location":"docker/#packer-docker","text":"Using Packer to build Docker might seem a bit strange at first. Aren't Dockerfiles a standard for building Docker containers? I'd be thinking this'll never fly with my JS developers? But It can make sense. To build with a Dockerfile, and to save to it to a registry requires 2 steps, first to build with a CI tool like Gitlab/Circle and requires a Dockerfile and a process to manage logging into the registry and pushing to it. In Packer this entire process can be captured in one file, encapsulating all that's required, and as a bonus the whole process is runnable locally. Which make for quick debug cycle times. Using Packer also allows the use and re-use of scripts and to use Ansible role and playbooks as used in other platform builds.","title":"Packer-Docker"},{"location":"docker/#a-very-basic-packer-docker-template","text":"Starting really simple with packer-docker-ubuntu.json . { \"_comment\" : \"Docker\" , \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"ubuntu\" , \"export_path\" : \"image.tar\" } ] } Building this template with.. $ packer build packer-docker-ubuntu.json docker output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: ubuntu docker: Using default tag: latest docker: latest: Pulling from library/ubuntu docker: 898c46f3b1a1: Pulling fs layer docker: 63366dfa0a50: Pulling fs layer docker: 041d4cd74a92: Pulling fs layer docker: 6e1bee0f8701: Pulling fs layer docker: 6e1bee0f8701: Waiting docker: 041d4cd74a92: Verifying Checksum docker: 041d4cd74a92: Download complete docker: 63366dfa0a50: Verifying Checksum docker: 63366dfa0a50: Download complete docker: 6e1bee0f8701: Download complete docker: 898c46f3b1a1: Verifying Checksum docker: 898c46f3b1a1: Download complete docker: 898c46f3b1a1: Pull complete docker: 63366dfa0a50: Pull complete docker: 041d4cd74a92: Pull complete docker: 6e1bee0f8701: Pull complete docker: Digest: sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8 docker: Status: Downloaded newer image for ubuntu:latest ==> docker: Starting docker container... docker: Run command: docker run -v /home/jim/.packer.d/tmp:/packer-files -d -i -t ubuntu /bin/bash docker: Container ID: 2dc59e83c81eef5feb0bfa5370a1616bdae5effa93b01ca6378716815582425d ==> docker: Using docker communicator to connect: 172.17.0.2 ==> docker: Exporting the container ==> docker: Killing the container: 2dc59e83c81eef5feb0bfa5370a1616bdae5effa93b01ca6378716815582425d Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Exported Docker file: image.tar OK so that's Not that useful yet. Yet.","title":"A very basic Packer Docker template"},{"location":"docker/#tagging-with-a-post-processor","text":"So far you have seen demonstrated that a build of a container with Packer, but it's not good for much yet. Next is to start plugging in other components. First up is a post-processor, used to tag the container. Enter the example packer-docker-tag-empty.json { \"variables\" : { \"tag\" : \"{{env `BUILD_NUMBER`}}\" }, \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"ubuntu\" , \"commit\" : true } ], \"post-processors\" : [ { \"type\" : \"docker-tag\" , \"tag\" : \"{{user `tag`}}\" } ] } Still not exactly Rocket Science yet, is it. Trying it out. $ packer build -var tag=2 packer-docker-tag-empty.json docker output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: ubuntu docker: Using default tag: latest docker: latest: Pulling from library/ubuntu docker: Digest: sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8 docker: Status: Image is up to date for ubuntu:latest ==> docker: Starting docker container... docker: Run command: docker run -v /home/jim/.packer.d/tmp:/packer-files -d -i -t ubuntu /bin/bash docker: Container ID: fe20e550df8e38e99a4a87ffe1836c646eecce7cf31db5da67f55d80c6ba54f8 ==> docker: Using docker communicator to connect: 172.17.0.2 ==> docker: Committing the container docker: Image ID: sha256:bd89cee10dc09fd28dd5cf1d00fc62fd659db014ef96b2d62a74eccf88c4b70d ==> docker: Killing the container: fe20e550df8e38e99a4a87ffe1836c646eecce7cf31db5da67f55d80c6ba54f8 ==> docker: Running post-processor: docker-tag docker (docker-tag): Tagging image: sha256:bd89cee10dc09fd28dd5cf1d00fc62fd659db014ef96b2d62a74eccf88c4b70d docker (docker-tag): Repository: dave:2 Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Imported Docker image: sha256:bd89cee10dc09fd28dd5cf1d00fc62fd659db014ef96b2d62a74eccf88c4b70d --> docker: Imported Docker image: dave:2 So now we have a tagged Dave:2 container. Contain your excitement. Ok let's move on.","title":"Tagging with a post-processor"},{"location":"docker/#building-into-an-aws-registry","text":"This next example will do everything, ok the main stuff for a container, build, tag and push to ECR. Running packer build .\\packer-docker-aws-ecr.json { \"_comment\" : \"Docker AWS ECR\" , \"variables\" : { \"tag\" : \"{{env `BUILD_NUMBER`}}\" , \"repository\" : \"123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container\" , \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"login_server\" : \"https://123456789012.dkr.ecr.eu-west-1.amazonaws.com/\" }, \"_comment\" : \"SSH needs to be on container for shell provisioner\" , \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"hashicorp/terraform\" , \"commit\" : true } ], \"post-processors\" : [ [ { \"type\" : \"docker-tag\" , \"tag\" : \"{{user `tag`}}\" , \"repository\" : \"{{user `repository`}}\" }, { \"type\" : \"docker-push\" , \"ecr_login\" : true , \"aws_access_key\" : \"{{user `aws_access_key`}}\" , \"aws_secret_key\" : \"{{user `aws_secret_key`}}\" , \"login_server\" : \"{{user `login_server`}}\" } ] ] } When this runs in Packer... docker output will be in this color. ==> docker: Creating a temporary directory for sharing data... ==> docker: Pulling Docker image: hashicorp/terraform docker: Using default tag: latest docker: latest: Pulling from hashicorp/terraform docker: Digest: sha256:330bef7401e02e757e6fa2de69f398fd29fcbfafe2a3b9e8f150486fbcd7915b docker: Status: Image is up to date for hashicorp/terraform:latest ==> docker: Starting docker container... docker: Run command: docker run -v /c/Users/james.woolfenden/packer.d/tmp:/packer-files -d -i -t hashicorp/terraform /bin/bash docker: Container ID: fa8f17d078f1978cda0a7b0c9352e7afffc8a2cb57db425faff3cc8f9816cf5d ==> docker: Using docker communicator to connect: ==> docker: Committing the container docker: Image ID: sha256:759f4b14fe9d477a7a6492ff9c1becd77e3d8b1e01a89f07275c8d72f3e7907d ==> docker: Killing the container: fa8f17d078f1978cda0a7b0c9352e7afffc8a2cb57db425faff3cc8f9816cf5d ==> docker: Running post-processor: docker-tag docker (docker-tag): Tagging image: sha256:759f4b14fe9d477a7a6492ff9c1becd77e3d8b1e01a89f07275c8d72f3e7907d docker (docker-tag): Repository: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 ==> docker: Running post-processor: docker-push docker (docker-push): Fetching ECR credentials... docker (docker-push): Logging in... docker (docker-push): Login Succeeded docker (docker-push): Pushing: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 docker (docker-push): The push refers to repository [123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container] docker (docker-push): 6ad0db50b392: Preparing docker (docker-push): 1eeb4487de94: Preparing docker (docker-push): 9094b98cdb25: Preparing docker (docker-push): 503e53e365f3: Preparing docker (docker-push): 9094b98cdb25: Layer already exists docker (docker-push): 1eeb4487de94: Layer already exists docker (docker-push): 503e53e365f3: Layer already exists docker (docker-push): 6ad0db50b392: Pushed docker (docker-push): 5: digest: sha256:82de02fbaab0a2c35d033cae5278c59e1d8e65ee2bb4cb5a57b803536b305a3c size: 1155 docker (docker-push): Logging out... docker (docker-push): Removing login credentials for 123456789012.dkr.ecr.eu-west-1.amazonaws.com Build 'docker' finished. ==> Builds finished. The artifacts of successful builds are: --> docker: Imported Docker image: sha256:759f4b14fe9d477a7a6492ff9c1becd77e3d8b1e01a89f07275c8d72f3e7907d --> docker: Imported Docker image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 --> docker: Imported Docker image: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 While that's great, we added nothing to the container so it doesn't prove that much yet. Sure it connected to ECR and pushed the container, but we added nothing to it, for that, we need to add some Provisioners. Packer builds containers will only work with Docker images that already have SSH on them. Reading this will save you a lot of time. I've changed the base image from hashicorp/terraform to ebiwd/alpine-ssh , and added terraform to its package list. This the shell script it runs on the Provisioner install_awscli.sh . #!/bin/sh apk update apk add --update nodejs-current nodejs-npm terraform apk add --update python python-dev py-pip build-base pip install --upgrade pip pip install awscli","title":"Building into an AWS registry"},{"location":"docker/#provisioners","text":"This next example combines the lot, it's imaginatively called packer-docker-aws-ecr-shell.json { \"_comment\" : \"Docker AWS ECR\" , \"variables\" : { \"tag\" : \"{{env `BUILD_NUMBER`}}\" , \"repository\" : \"123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container\" , \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"login_server\" : \"https://123456789012.dkr.ecr.eu-west-1.amazonaws.com/\" }, \"_comment\" : \"SSH needs to be on container for shell provisioner\" , \"builders\" : [ { \"type\" : \"docker\" , \"image\" : \"ebiwd/alpine-ssh\" , \"commit\" : true } ], \"_comment\" : \"probs bash needs to be there\" , \"provisioners\" : [ { \"type\" : \"shell\" , \"inline\" : [ \"apk add bash\" ] }, { \"type\" : \"shell\" , \"script\" : \"install-awscli.sh\" } ], \"post-processors\" : [ [ { \"type\" : \"docker-tag\" , \"tag\" : \"{{user `tag`}}\" , \"repository\" : \"{{user `repository`}}\" }, { \"type\" : \"docker-push\" , \"ecr_login\" : true , \"aws_access_key\" : \"{{user `aws_access_key`}}\" , \"aws_secret_key\" : \"{{user `aws_secret_key`}}\" , \"login_server\" : \"{{user `login_server`}}\" } ] ] } Let's built it: $ packer build . \\p acker-docker-aws-ecr.json docker output will be in this color. == > docker: Creating a temporary directory for sharing data... == > docker: Pulling Docker image: ebiwd/alpine-ssh docker: Using default tag: latest docker: latest: Pulling from ebiwd/alpine-ssh docker: 59265c40e257: Pulling fs layer docker: a621cd180b0b: Pulling fs layer docker: 1388eaedce13: Pulling fs layer docker: 75cc2d2e3f13: Pulling fs layer docker: b4ac607039c6: Pulling fs layer docker: 75cc2d2e3f13: Waiting docker: b4ac607039c6: Waiting docker: 59265c40e257: Verifying Checksum docker: 59265c40e257: Download complete docker: 59265c40e257: Pull complete docker: 75cc2d2e3f13: Verifying Checksum docker: 75cc2d2e3f13: Download complete docker: b4ac607039c6: Download complete docker: a621cd180b0b: Verifying Checksum docker: a621cd180b0b: Download complete docker: a621cd180b0b: Pull complete docker: 1388eaedce13: Verifying Checksum docker: 1388eaedce13: Download complete docker: 1388eaedce13: Pull complete docker: 75cc2d2e3f13: Pull complete docker: b4ac607039c6: Pull complete docker: Digest: sha256:6d994480f495fc7ddc16d126e4c41f9fc0153363b6f9e686f50d47039078615c docker: Status: Downloaded newer image for ebiwd/alpine-ssh:latest == > docker: Starting docker container... docker: Run command: docker run -v /c/Users/SOMEUSER/packer.d/tmp:/packer-files -d -i -t ebiwd/alpine-ssh /bin/bash docker: Container ID: 5fbda610d9117b5a2c20f5da6e77b4cb230114c44fbf79b168a30e32e3bc26f2 == > docker: Using docker communicator to connect: 172 .17.0.2 == > docker: Provisioning with shell script: C: \\U sers \\S OMEUSER \\A ppData \\L ocal \\T emp \\p acker-shell350255759 docker: OK: 68 MiB in 39 packages docker: WARNING: Ignoring APKINDEX.84815163.tar.gz: No such file or directory docker: WARNING: Ignoring APKINDEX.24d64ab1.tar.gz: No such file or directory == > docker: Provisioning with shell script: install-awscli.sh docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz docker: v3.6.5-5-geec223036a [ http://dl-cdn.alpinelinux.org/alpine/v3.6/main ] docker: v3.6.5-4-g1bf6e4dfc6 [ http://dl-cdn.alpinelinux.org/alpine/v3.6/community ] docker: OK: 8449 distinct packages available docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz docker: ( 1 /8 ) Installing libcrypto1.0 ( 1 .0.2r-r0 ) docker: ( 2 /8 ) Installing libgcc ( 6 .3.0-r4 ) docker: ( 3 /8 ) Installing http-parser ( 2 .7.1-r1 ) docker: ( 4 /8 ) Installing libssl1.0 ( 1 .0.2r-r0 ) docker: ( 5 /8 ) Installing libstdc++ ( 6 .3.0-r4 ) docker: ( 6 /8 ) Installing libuv ( 1 .11.0-r1 ) docker: ( 7 /8 ) Installing nodejs-current ( 7 .10.1-r1 ) docker: ( 8 /8 ) Installing nodejs-npm ( 6 .10.3-r2 ) docker: Executing busybox-1.26.2-r11.trigger docker: OK: 110 MiB in 47 packages docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz docker: fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz docker: ( 1 /19 ) Upgrading musl ( 1 .1.16-r14 -> 1 .1.16-r15 ) docker: ( 2 /19 ) Installing binutils-libs ( 2 .30-r1 ) docker: ( 3 /19 ) Installing binutils ( 2 .30-r1 ) docker: ( 4 /19 ) Installing gmp ( 6 .1.2-r0 ) docker: ( 5 /19 ) Installing isl ( 0 .17.1-r0 ) docker: ( 6 /19 ) Installing libgomp ( 6 .3.0-r4 ) docker: ( 7 /19 ) Installing libatomic ( 6 .3.0-r4 ) docker: ( 8 /19 ) Installing pkgconf ( 1 .3.7-r0 ) docker: ( 9 /19 ) Installing mpfr3 ( 3 .1.5-r0 ) docker: ( 10 /19 ) Installing mpc1 ( 1 .0.3-r0 ) docker: ( 11 /19 ) Installing gcc ( 6 .3.0-r4 ) docker: ( 12 /19 ) Installing musl-dev ( 1 .1.16-r15 ) docker: ( 13 /19 ) Installing libc-dev ( 0 .7.1-r0 ) docker: ( 14 /19 ) Installing g++ ( 6 .3.0-r4 ) docker: ( 15 /19 ) Installing make ( 4 .2.1-r0 ) docker: ( 16 /19 ) Installing fortify-headers ( 0 .8-r0 ) docker: ( 17 /19 ) Installing build-base ( 0 .5-r0 ) docker: ( 18 /19 ) Upgrading musl-utils ( 1 .1.16-r14 -> 1 .1.16-r15 ) docker: ( 19 /19 ) Installing python2-dev ( 2 .7.15-r0 ) docker: Executing busybox-1.26.2-r11.trigger docker: OK: 274 MiB in 64 packages docker: Collecting pip docker: Downloading https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl ( 1 .4MB ) docker: Installing collected packages: pip docker: Found existing installation: pip 9 .0.1 docker: Uninstalling pip-9.0.1: docker: Successfully uninstalled pip-9.0.1 docker: Successfully installed pip-19.0.3 docker: DEPRECATION: Python 2 .7 will reach the end of its life on January 1st, 2020 . Please upgrade your Python as Python 2 .7 won 't be maintained after that date. A future version of pip will drop support for Python 2.7. docker: Requirement already satisfied: awscli in /usr/lib/python2.7/site-packages (1.16.106) docker: Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/lib/python2.7/site-packages (from awscli) (0.2.0) docker: Requirement already satisfied: docutils>=0.10 in /usr/lib/python2.7/site-packages (from awscli) (0.14) docker: Requirement already satisfied: botocore==1.12.96 in /usr/lib/python2.7/site-packages (from awscli) (1.12.96) docker: Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/lib/python2.7/site-packages (from awscli) (3.13) docker: Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /usr/lib/python2.7/site-packages (from awscli) (0.3.9) docker: Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/lib/python2.7/site-packages (from awscli) (3.4.2) docker: Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/lib/python2.7/site-packages (from s3transfer<0.3.0,>=0.2.0->awscli) (3.2.0) docker: Requirement already satisfied: urllib3<1.25,>=1.20; python_version == \"2.7\" in /usr/lib/python2.7/site-packages (from botocore==1.12.96->awscli) (1.24.1) docker: Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/lib/python2.7/site-packages (from botocore==1.12.96->awscli) (2.8.0) docker: Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/lib/python2.7/site-packages (from botocore==1.12.96->awscli) (0.9.3) docker: Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python2.7/site-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.5) docker: Requirement already satisfied: six>=1.5 in /usr/lib/python2.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore==1.12.96->awscli) (1.12.0) ==> docker: Committing the container docker: Image ID: sha256:8af22b4e91c0e6ebc53e309ac3d803c989e7007c9e6c3f57dadeda0d059006d6 ==> docker: Killing the container: 5fbda610d9117b5a2c20f5da6e77b4cb230114c44fbf79b168a30e32e3bc26f2 ==> docker: Running post-processor: docker-tag docker (docker-tag): Tagging image: sha256:8af22b4e91c0e6ebc53e309ac3d803c989e7007c9e6c3f57dadeda0d059006d6 docker (docker-tag): Repository: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 ==> docker: Running post-processor: docker-push docker (docker-push): Fetching ECR credentials... docker (docker-push): Logging in... docker (docker-push): Login Succeeded docker (docker-push): Pushing: 123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 docker (docker-push): The push refers to repository [123456789012.dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container] docker (docker-push): 17073917cb42: Preparing docker (docker-push): 96287b9167c6: Preparing docker (docker-push): 8619e10b942a: Preparing docker (docker-push): e74e4c2ec07f: Preparing docker (docker-push): 780b2a6e852d: Preparing docker (docker-push): e79522dce35e: Preparing docker (docker-push): e79522dce35e: Waiting docker (docker-push): 96287b9167c6: Pushed docker (docker-push): e79522dce35e: Pushed docker (docker-push): 8619e10b942a: Pushed docker (docker-push): 780b2a6e852d: Pushed docker (docker-push): e74e4c2ec07f: Pushed docker (docker-push): 17073917cb42: Pushed docker (docker-push): 5: digest: sha256:5407f913fb6493f6d67079b562bfc570775c4a649eaf1ee58c6151abfea0b139 size: 1582 docker (docker-push): Logging out... docker (docker-push): Removing login credentials for 123456789012.dkr.ecr.eu-west-1.amazonaws.com Build ' docker ' finished. == > Builds finished. The artifacts of successful builds are: --> docker: Imported Docker image: sha256:8af22b4e91c0e6ebc53e309ac3d803c989e7007c9e6c3f57dadeda0d059006d6 --> docker: Imported Docker image: 123456789012 .dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 --> docker: Imported Docker image: 123456789012 .dkr.ecr.eu-west-1.amazonaws.com/aws-codebuild-container:5 So that's an example of nearly every key component to build your Dockerfile. All that's required is a tool to host it in. Note Packer build only works with docker images that already have SSH on them. Shell Provisioners - Bash scripts only run if you have Bash installed in your container. Packer Docker builds can't build from 'scratch'. Packer Provisioners use SSH, images like a Base Alpine will fail, so you can only base your containers that have SSH installed. No Layer caching.","title":"Provisioners"},{"location":"extending/","text":"Third Party Extensions The comment provisioner is an example of a third party extension for Packer, and is a little bit of fun, so what's not to like! If you have the Golang Build environment already set-up that is. Clone the repository and build the executable. git clone https : //github.com/SwampDragons/packer-provisioner-comment cd packer - provisioner - comment go mod init main go build mkdir ~ / . packer . d / plugins mv main ~ / . packer . d / plugins / packer - provisioner - comment And move it to the plugin directory. If you built on Windows, that last 2 lines should be: mkdir \"$env:APPDATA/packer.d/plugins\" mv main . exe \"$env:APPDATA/packer.d/plugins/packer-provisioner-comment.exe\" But moving the build executable plugin to my user profile worked for me: mkdir \"$env:USERPROFILE/packer.d/plugins\" mv main . exe \"$env:USERPROFILE/packer.d/plugins/packer-provisioner-comment.exe\" Taking my example from https://github.com/SwampDragons/packer-provisioner-comment as bubble.json { \"builders\" : [ { \"type\" : \"null\" , \"communicator\" : \"none\" } ], \"provisioners\" : [ { \"type\" : \"comment\" , \"comment\" : \"Begin\" , \"ui\" : true , \"bubble_text\" : true }, { \"type\" : \"shell-local\" , \"inline\" : [ \"echo \\\"This is a shell script\\\"\" ] }, { \"type\" : \"comment\" , \"comment\" : \"In the middle of Provisioning run\" , \"ui\" : true }, { \"type\" : \"shell-local\" , \"inline\" : [ \"echo \\\"This is another shell script\\\"\" ] }, { \"type\" : \"comment\" , \"comment\" : \"this comment is invisible and won't go to the UI\" }, { \"type\" : \"comment\" , \"comment\" : \"End\" , \"ui\" : true , \"bubble_text\" : true } ] } and running: $packer build . \\b ubble.json null output will be in this color. == > null: ____ _ == > null: | __ ) ___ __ _ ( _ ) _ __ == > null: | _ \\ / _ \\ / _ ` | | | | '_ \\ ==> null: | |_) | | __/ | (_| | | | | | | | ==> null: |____/ \\___| \\__, | |_| |_| |_| ==> null: |___/ ==> null: ==> null: Running local shell script: C:\\Users\\jim_w\\AppData\\Local\\Temp\\packer-shell332136251.cmd null: null: C:\\code\\packer>echo \"This is a shell script\" null: \"This is a shell script\" ==> null: In the middle of Provisioning run ==> null: Running local shell script: C:\\Users\\jim_w\\AppData\\Local\\Temp\\packer-shell954744271.cmd null: null: C:\\code\\packer>echo \"This is another shell script\" null: \"This is another shell script\" ==> null: _____ _ ==> null: | ____| _ __ __| | ==> null: | _| | ' _ \\ / _ ` | == > null: | | ___ | | | | | ( _ | | == > null: | _____ | | _ | | _ | \\_ _,_ | == > null: Build 'null' finished. == > Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder I can see me liking and using this one more. Extending Packer https://www.packer.io/docs/extending/plugins.html","title":"Extending"},{"location":"extending/#third-party-extensions","text":"The comment provisioner is an example of a third party extension for Packer, and is a little bit of fun, so what's not to like! If you have the Golang Build environment already set-up that is. Clone the repository and build the executable. git clone https : //github.com/SwampDragons/packer-provisioner-comment cd packer - provisioner - comment go mod init main go build mkdir ~ / . packer . d / plugins mv main ~ / . packer . d / plugins / packer - provisioner - comment And move it to the plugin directory. If you built on Windows, that last 2 lines should be: mkdir \"$env:APPDATA/packer.d/plugins\" mv main . exe \"$env:APPDATA/packer.d/plugins/packer-provisioner-comment.exe\" But moving the build executable plugin to my user profile worked for me: mkdir \"$env:USERPROFILE/packer.d/plugins\" mv main . exe \"$env:USERPROFILE/packer.d/plugins/packer-provisioner-comment.exe\" Taking my example from https://github.com/SwampDragons/packer-provisioner-comment as bubble.json { \"builders\" : [ { \"type\" : \"null\" , \"communicator\" : \"none\" } ], \"provisioners\" : [ { \"type\" : \"comment\" , \"comment\" : \"Begin\" , \"ui\" : true , \"bubble_text\" : true }, { \"type\" : \"shell-local\" , \"inline\" : [ \"echo \\\"This is a shell script\\\"\" ] }, { \"type\" : \"comment\" , \"comment\" : \"In the middle of Provisioning run\" , \"ui\" : true }, { \"type\" : \"shell-local\" , \"inline\" : [ \"echo \\\"This is another shell script\\\"\" ] }, { \"type\" : \"comment\" , \"comment\" : \"this comment is invisible and won't go to the UI\" }, { \"type\" : \"comment\" , \"comment\" : \"End\" , \"ui\" : true , \"bubble_text\" : true } ] } and running: $packer build . \\b ubble.json null output will be in this color. == > null: ____ _ == > null: | __ ) ___ __ _ ( _ ) _ __ == > null: | _ \\ / _ \\ / _ ` | | | | '_ \\ ==> null: | |_) | | __/ | (_| | | | | | | | ==> null: |____/ \\___| \\__, | |_| |_| |_| ==> null: |___/ ==> null: ==> null: Running local shell script: C:\\Users\\jim_w\\AppData\\Local\\Temp\\packer-shell332136251.cmd null: null: C:\\code\\packer>echo \"This is a shell script\" null: \"This is a shell script\" ==> null: In the middle of Provisioning run ==> null: Running local shell script: C:\\Users\\jim_w\\AppData\\Local\\Temp\\packer-shell954744271.cmd null: null: C:\\code\\packer>echo \"This is another shell script\" null: \"This is another shell script\" ==> null: _____ _ ==> null: | ____| _ __ __| | ==> null: | _| | ' _ \\ / _ ` | == > null: | | ___ | | | | | ( _ | | == > null: | _____ | | _ | | _ | \\_ _,_ | == > null: Build 'null' finished. == > Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder I can see me liking and using this one more. Extending Packer https://www.packer.io/docs/extending/plugins.html","title":"Third Party Extensions"},{"location":"functions/","text":"functions template_dir This is a built in Packer function, and it returns the path to the current file. If you have to reference another file in a provisioner or as in the line below to AWS user data script file: \"user_data_file\" : \"{{template_dir}}/bootstrap_win.txt\" timestamp Always useful, to make name and strings unique. \"{{timestamp}}\"","title":"Functions"},{"location":"functions/#functions","text":"","title":"functions"},{"location":"functions/#template_dir","text":"This is a built in Packer function, and it returns the path to the current file. If you have to reference another file in a provisioner or as in the line below to AWS user data script file: \"user_data_file\" : \"{{template_dir}}/bootstrap_win.txt\"","title":"template_dir"},{"location":"functions/#timestamp","text":"Always useful, to make name and strings unique. \"{{timestamp}}\"","title":"timestamp"},{"location":"hcl2/","text":"HCL2 At the Beginning of Feb 2020 Hashicorp announced that they were supporting HCL2 for Packer files. They're not dropping support for json. What's different and what's better The quick answer is that Packer files will a look more like Terraform and so it looks more like code than data- json . Support for all features found in Packer file is not yet complete, but to give you an idea of how they compare, here's a short traditional Windows example with the new format. .\\examples\\hcl2vjson1\\json1\\winserver2019-aws.json { \"variables\" : { \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"build_number\" : \"{{env `BUILD_NUMBER`}}\" , \"instance_type\" : \"t2.micro\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"access_key\" : \"{{user `aws_access_key`}}\" , \"secret_key\" : \"{{user `aws_secret_key`}}\" , \"region\" : \"{{user `aws_region`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"Windows_Server-2019-English-Full-Base*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"self\" , \"amazon\" ], \"most_recent\" : true }, \"instance_type\" : \"{{ user `instance_type` }}\" , \"user_data_file\" : \"{{template_dir}}/bootstrap_win.txt\" , \"communicator\" : \"winrm\" , \"winrm_username\" : \"Administrator\" , \"winrm_timeout\" : \"10m\" , \"winrm_password\" : \"SuperS3cr3t!!!!\" , \"ami_name\" : \"Base v{{user `version`}} Windows2019\" , \"ami_description\" : \"Windows 2019 Base\" , \"ami_virtualization_type\" : \"hvm\" , \"vpc_id\" : \"{{user `vpc_id`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" } ], \"provisioners\" : [ { \"type\" : \"powershell\" , \"inline\" : [ \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))|out-null\" ] }, { \"type\" : \"powershell\" , \"inline\" : [ \"choco install javaruntime -y -force\" ] } ] } In HCL2 this comes[the file extension is required]: .\\examples\\hcl2vjson1\\hcl2\\amazon-ebs.Windows2019.pkr.hcl source \"amazon-ebs\" \"Windows2019\" { region= \"\" instance_type= \"t2.micro\" source_ami_filter { filters { virtualization-type= \"hvm\" name=\"Windows_Server-2019-English-Full-Base*\" root-device-type= \"ebs\" } most_recent= true owners= [\"amazon\"] } ami_name= \"Base v1 Windows2019 {{timestamp}}\" ami_description= \"Windows 2019 Base\" associate_public_ip_address=\"true\" user_data_file= \"./HCL2/bootstrap_win.txt\" communicator= \"winrm\" winrm_username= \"Administrator\" winrm_timeout= \"10m\" winrm_password=\"SuperS3cr3t!!!!\" #using spot market as cheaper spot_price=\"0\" #if empty it uses the default vpc, COMMENTS!!!! vpc_id= \"\" subnet_id=\"\" } build { sources =[ \"source.amazon-ebs.Windows2019\" ] provisioner \"powershell\" { inline = [\"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))|out-null\" ,\"choco install javaruntime -y -force\"] } } Build folders Packer can now target a folder and build all the .hcl files in one folder. We can separate the build block into: build.Windows2019.pkr.hcl build { sources =[ \"source.amazon-ebs.Windows2019\" ] provisioner \"powershell\" { inline = [\"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))|out-null\" ,\"choco install javaruntime -y -force\"] } } And build by specifying the folder: packer build .\\hcl2\\ Separating built type from Provisioners should make for better re-use. Findings So what has changed, besides the brackets and commas? There's no support for variables yet [it's more alpha than beta] and functions and you can only have one provisioner of each type as yet, but I do think the look is clearer. It looks like they plan to be able to pass parameters around like you can in Terraform. So this should be a good thing as is the splitting into multiple files. to be continued...","title":"HCL2"},{"location":"hcl2/#hcl2","text":"At the Beginning of Feb 2020 Hashicorp announced that they were supporting HCL2 for Packer files. They're not dropping support for json.","title":"HCL2"},{"location":"hcl2/#whats-different-and-whats-better","text":"The quick answer is that Packer files will a look more like Terraform and so it looks more like code than data- json . Support for all features found in Packer file is not yet complete, but to give you an idea of how they compare, here's a short traditional Windows example with the new format. .\\examples\\hcl2vjson1\\json1\\winserver2019-aws.json { \"variables\" : { \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"build_number\" : \"{{env `BUILD_NUMBER`}}\" , \"instance_type\" : \"t2.micro\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"access_key\" : \"{{user `aws_access_key`}}\" , \"secret_key\" : \"{{user `aws_secret_key`}}\" , \"region\" : \"{{user `aws_region`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"Windows_Server-2019-English-Full-Base*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"self\" , \"amazon\" ], \"most_recent\" : true }, \"instance_type\" : \"{{ user `instance_type` }}\" , \"user_data_file\" : \"{{template_dir}}/bootstrap_win.txt\" , \"communicator\" : \"winrm\" , \"winrm_username\" : \"Administrator\" , \"winrm_timeout\" : \"10m\" , \"winrm_password\" : \"SuperS3cr3t!!!!\" , \"ami_name\" : \"Base v{{user `version`}} Windows2019\" , \"ami_description\" : \"Windows 2019 Base\" , \"ami_virtualization_type\" : \"hvm\" , \"vpc_id\" : \"{{user `vpc_id`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" } ], \"provisioners\" : [ { \"type\" : \"powershell\" , \"inline\" : [ \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))|out-null\" ] }, { \"type\" : \"powershell\" , \"inline\" : [ \"choco install javaruntime -y -force\" ] } ] } In HCL2 this comes[the file extension is required]: .\\examples\\hcl2vjson1\\hcl2\\amazon-ebs.Windows2019.pkr.hcl source \"amazon-ebs\" \"Windows2019\" { region= \"\" instance_type= \"t2.micro\" source_ami_filter { filters { virtualization-type= \"hvm\" name=\"Windows_Server-2019-English-Full-Base*\" root-device-type= \"ebs\" } most_recent= true owners= [\"amazon\"] } ami_name= \"Base v1 Windows2019 {{timestamp}}\" ami_description= \"Windows 2019 Base\" associate_public_ip_address=\"true\" user_data_file= \"./HCL2/bootstrap_win.txt\" communicator= \"winrm\" winrm_username= \"Administrator\" winrm_timeout= \"10m\" winrm_password=\"SuperS3cr3t!!!!\" #using spot market as cheaper spot_price=\"0\" #if empty it uses the default vpc, COMMENTS!!!! vpc_id= \"\" subnet_id=\"\" } build { sources =[ \"source.amazon-ebs.Windows2019\" ] provisioner \"powershell\" { inline = [\"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))|out-null\" ,\"choco install javaruntime -y -force\"] } }","title":"What's different and what's better"},{"location":"hcl2/#build-folders","text":"Packer can now target a folder and build all the .hcl files in one folder. We can separate the build block into: build.Windows2019.pkr.hcl build { sources =[ \"source.amazon-ebs.Windows2019\" ] provisioner \"powershell\" { inline = [\"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))|out-null\" ,\"choco install javaruntime -y -force\"] } } And build by specifying the folder: packer build .\\hcl2\\ Separating built type from Provisioners should make for better re-use.","title":"Build folders"},{"location":"hcl2/#findings","text":"So what has changed, besides the brackets and commas? There's no support for variables yet [it's more alpha than beta] and functions and you can only have one provisioner of each type as yet, but I do think the look is clearer. It looks like they plan to be able to pass parameters around like you can in Terraform. So this should be a good thing as is the splitting into multiple files. to be continued...","title":"Findings"},{"location":"help/","text":"Help If its been useful, let me know. If it out of date or broken also. I'll appreciate it the info. Or If you think somethings missing or contribute? Got a question? File a GitHub issue . Contributing Bug Reports & Feature Requests Please use the issue tracker to report any bugs or file feature requests. Copyrights Copyright \u00a9 2019-2019 James Woolfenden License Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contributors James Woolfenden","title":"Help"},{"location":"help/#help","text":"If its been useful, let me know. If it out of date or broken also. I'll appreciate it the info. Or If you think somethings missing or contribute? Got a question? File a GitHub issue .","title":"Help"},{"location":"help/#contributing","text":"","title":"Contributing"},{"location":"help/#bug-reports-feature-requests","text":"Please use the issue tracker to report any bugs or file feature requests.","title":"Bug Reports &amp; Feature Requests"},{"location":"help/#copyrights","text":"Copyright \u00a9 2019-2019 James Woolfenden","title":"Copyrights"},{"location":"help/#license","text":"Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"help/#contributors","text":"James Woolfenden","title":"Contributors"},{"location":"imagebuilder/","text":"The New AWS Imagebuilder AWS Imagebuilder was announced and released Globally at 2019's Re:invent. https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-ec2-image-builder There is now a new section in the AWS console UI, which looks initially like: Once you click create or edit an existing pipeline: There is a triggers option, if you enable this it will run this pipeline on the release of new base image, which will prove handy for patches. This will Initiate a new image build when there are updates to your selected base image version. Support for Amazon Linux and WIndows Server is built in, but to build for other OS/images you'll need to add the SSM service to your base AMI. The other 2 main part are the support for build components and tests , think Provisioners if you like. Build components You can browse to, or add your own or use others published components or click to create build component This launches a UI, there is an included example to get you started: name : HelloWorldTestingDocument description : This is hello world testing document. schemaVersion : 1.0 phases : - name : build steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Build.\" - name : validate steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Validate.\" - name : test steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Test.\" There are already a number of defined build components and adding your own bash or ps1 files looks straight forward. Questions Can I, or how do I reference Ansible playbooks. Git repo for components updates and modifications At the moment you upload all you related files to S3, I would have though a git repo with associated triggers would be good. YAML component The syntax for was the component support Tests You can create, browse your own or add others pre-built test components as well The support for built in tests looks promising. Click create test And again we get a sample populated to clue us in. name : HelloWorldTestingDocument description : This is hello world testing document. schemaVersion : 1.0 phases : - name : test steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Test.\" There's a skeleton example included here. Pipeline With a component added you also need to have an IAM role for the pipeline, I added 2 managed polices but you may want to trim excess permissions: IAM Permissions With a role added you now have a minimal imagebuilder pipeline you can run. Invoke and wait You can them manually invoke. There is no log output so don't get to see the output as it runs from the UI, they went for a wait and see approach. As you can see it looks a lot like codebuild, however it does support versioned artefacts for your AMIS which is great to see. Hopefully it will easy to pick up the versions for use with launching instances. Questions How does this integrate with Amazon Inspector? Why do we now have 2 different build pipelines, with this and AWS Codepipeline. Running from the cli https://docs.aws.amazon.com/imagebuilder/latest/userguide/managing-image-builder-cli.html","title":"Imagebuilder"},{"location":"imagebuilder/#the-new-aws-imagebuilder","text":"AWS Imagebuilder was announced and released Globally at 2019's Re:invent. https://aws.amazon.com/about-aws/whats-new/2019/12/introducing-ec2-image-builder There is now a new section in the AWS console UI, which looks initially like: Once you click create or edit an existing pipeline: There is a triggers option, if you enable this it will run this pipeline on the release of new base image, which will prove handy for patches. This will Initiate a new image build when there are updates to your selected base image version. Support for Amazon Linux and WIndows Server is built in, but to build for other OS/images you'll need to add the SSM service to your base AMI. The other 2 main part are the support for build components and tests , think Provisioners if you like.","title":"The New AWS Imagebuilder"},{"location":"imagebuilder/#build-components","text":"You can browse to, or add your own or use others published components or click to create build component This launches a UI, there is an included example to get you started: name : HelloWorldTestingDocument description : This is hello world testing document. schemaVersion : 1.0 phases : - name : build steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Build.\" - name : validate steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Validate.\" - name : test steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Test.\" There are already a number of defined build components and adding your own bash or ps1 files looks straight forward. Questions Can I, or how do I reference Ansible playbooks. Git repo for components updates and modifications At the moment you upload all you related files to S3, I would have though a git repo with associated triggers would be good. YAML component The syntax for was the component support","title":"Build components"},{"location":"imagebuilder/#tests","text":"You can create, browse your own or add others pre-built test components as well The support for built in tests looks promising. Click create test And again we get a sample populated to clue us in. name : HelloWorldTestingDocument description : This is hello world testing document. schemaVersion : 1.0 phases : - name : test steps : - name : HelloWorldStep action : ExecuteBash inputs : commands : - echo \"Hello World! Test.\" There's a skeleton example included here.","title":"Tests"},{"location":"imagebuilder/#pipeline","text":"With a component added you also need to have an IAM role for the pipeline, I added 2 managed polices but you may want to trim excess permissions:","title":"Pipeline"},{"location":"imagebuilder/#iam-permissions","text":"With a role added you now have a minimal imagebuilder pipeline you can run.","title":"IAM Permissions"},{"location":"imagebuilder/#invoke-and-wait","text":"You can them manually invoke. There is no log output so don't get to see the output as it runs from the UI, they went for a wait and see approach. As you can see it looks a lot like codebuild, however it does support versioned artefacts for your AMIS which is great to see. Hopefully it will easy to pick up the versions for use with launching instances. Questions How does this integrate with Amazon Inspector? Why do we now have 2 different build pipelines, with this and AWS Codepipeline.","title":"Invoke and wait"},{"location":"imagebuilder/#running-from-the-cli","text":"https://docs.aws.amazon.com/imagebuilder/latest/userguide/managing-image-builder-cli.html","title":"Running from the cli"},{"location":"installing-packer/","text":"Installing Packer Packer is a tool for creating images and multiple image types from a single file. Manual Installation instructions are well documented on the Packer website. Packer like Terraform, is frequently updated with new features, as well as fixes, which are well documented in its changelog https://github.com/hashicorp/packer/blob/master/CHANGELOG.md . Or you could submit your new feature or fix yourself. Automated Installation options ```mac tab=\"mac\" brew install packer ```powershell tab=\"powershell\" cinst packer ```bash tab=\"linux\" !/bin/bash set -exo curl https://keybase.io/hashicorp/pgp_keys.asc | gpg --import apt-get install unzip VERSION=\"1.5.2\" TOOL=\"packer\" EDITION=\"linux_amd64\" cd /usr/local/bin Download the binary and signature files. wget \" https://releases.hashicorp.com/ TOOL/ TOOL/ VERSION/ {TOOL}_ {TOOL}_ {VERSION} {EDITION}.zip\" wget \"https://releases.hashicorp.com/ {EDITION}.zip\" wget \"https://releases.hashicorp.com/ TOOL/ VERSION/ VERSION/ {TOOL} {VERSION}_SHA256SUMS\" wget \"https://releases.hashicorp.com/ {VERSION}_SHA256SUMS\" wget \"https://releases.hashicorp.com/ TOOL/ VERSION/ VERSION/ {TOOL}_${VERSION}_SHA256SUMS.sig\" Verify the signature file is untampered. gpg --verify \" {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS.sig\" \" {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS\" only check against your tool sed '/linux_amd64/!d' {TOOL}_ {TOOL}_ {VERSION} SHA256SUMS sed '/linux_amd64/!d' {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS > {TOOL}_ {TOOL}_ {VERSION} ${EDITION}_SHA256SUMS Verify the SHASUM matches the binary. shasum -a 256 -c \" {TOOL}_ {TOOL}_ {VERSION}_${EDITION}_SHA256SUMS\" unzip \" {TOOL}_ {TOOL}_ {VERSION} linux_amd64.zip\" rm \" {TOOL}_ {TOOL}_ {VERSION}_linux_amd64.zip\" rm \" {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS\" rm \" {TOOL}_ {TOOL}_ {VERSION} {EDITION}_SHA256SUMS\" rm \" {EDITION}_SHA256SUMS\" rm \" {TOOL}_${VERSION}_SHA256SUMS.sig\" \"${TOOL}\" --version Brew and Chocolatey repositories for Packer are usually up to date. Packer is published to **yum** and **apt-get** repositories but these installs require a substantial number of dependencies and can be quite out of date. At this time for Ubuntu 18 repository has v1.0.3 but v1.5.2 is current. ### Docker container You can also you use Packer from a container ```docker $docker pull hashicorp/packer docker images REPOSITORY TAG IMAGE ID CREATED SIZE hashicorp/packer latest 2453d5a18479 2 weeks ago 167MB You can run it much line the regular cmdline. $docker run 2453d5a18479 --version 1.35 Or $docker run -i -t hashicorp/packer:light validate You will need to pass in the Packer files by sharing the host folder into Packers Container. $docker run -v /c/code/book/packer/01-installing-packer/:/home/docker/ hashicorp/packer:light validate /home/docker/empty.json But when you run the build: $docker run -v /c/code/book/packer/01-installing-packer/:/home/docker/ hashicorp/packer:light build /home/docker/empty.json You get: Build 'docker' errored: exec: \"docker\": executable file not found in \\$PATH ==\\> Some builds didn't complete successfully and had errors: --\\> docker: exec: \"docker\": executable file not found in \\$PATH ==\\> Builds finished but no artifacts were created. Which is confusing until you realise/remember that all that's in that container is Packer and not Docker. So while you could use a Packer image to build your containers you'd have to add and customise your own anyway. Packer from source code You can build your own, if you are building from source, use at least version 1.12 of go-lang and follow the instructions from their readme. Assuming you have >= 1.12 golang https://github.com/hashicorp/packer/blob/master/.github/CONTRIBUTING.md#setting-up-go-to-work-on-packer Just run go get and wait. go get github.com/hashicorp/packer cd \\$gopath $ packer -version 1.35 Debugging You can run Packer in Debug mode using the -debug flag, it's fairly verbose but it is particularly useful. If you need to see which AWS call fails and/or to get the temporary SSH key that's can be used to connect to the box used to make your AMI? It'll happen.","title":"Installing"},{"location":"installing-packer/#installing-packer","text":"Packer is a tool for creating images and multiple image types from a single file. Manual Installation instructions are well documented on the Packer website. Packer like Terraform, is frequently updated with new features, as well as fixes, which are well documented in its changelog https://github.com/hashicorp/packer/blob/master/CHANGELOG.md . Or you could submit your new feature or fix yourself.","title":"Installing Packer"},{"location":"installing-packer/#automated-installation-options","text":"```mac tab=\"mac\" brew install packer ```powershell tab=\"powershell\" cinst packer ```bash tab=\"linux\"","title":"Automated Installation options"},{"location":"installing-packer/#binbash","text":"set -exo curl https://keybase.io/hashicorp/pgp_keys.asc | gpg --import apt-get install unzip VERSION=\"1.5.2\" TOOL=\"packer\" EDITION=\"linux_amd64\" cd /usr/local/bin","title":"!/bin/bash"},{"location":"installing-packer/#download-the-binary-and-signature-files","text":"wget \" https://releases.hashicorp.com/ TOOL/ TOOL/ VERSION/ {TOOL}_ {TOOL}_ {VERSION} {EDITION}.zip\" wget \"https://releases.hashicorp.com/ {EDITION}.zip\" wget \"https://releases.hashicorp.com/ TOOL/ VERSION/ VERSION/ {TOOL} {VERSION}_SHA256SUMS\" wget \"https://releases.hashicorp.com/ {VERSION}_SHA256SUMS\" wget \"https://releases.hashicorp.com/ TOOL/ VERSION/ VERSION/ {TOOL}_${VERSION}_SHA256SUMS.sig\"","title":"Download the binary and signature files."},{"location":"installing-packer/#verify-the-signature-file-is-untampered","text":"gpg --verify \" {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS.sig\" \" {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS\"","title":"Verify the signature file is untampered."},{"location":"installing-packer/#only-check-against-your-tool","text":"sed '/linux_amd64/!d' {TOOL}_ {TOOL}_ {VERSION} SHA256SUMS sed '/linux_amd64/!d' {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS > {TOOL}_ {TOOL}_ {VERSION} ${EDITION}_SHA256SUMS","title":"only check against your tool"},{"location":"installing-packer/#verify-the-shasum-matches-the-binary","text":"shasum -a 256 -c \" {TOOL}_ {TOOL}_ {VERSION}_${EDITION}_SHA256SUMS\" unzip \" {TOOL}_ {TOOL}_ {VERSION} linux_amd64.zip\" rm \" {TOOL}_ {TOOL}_ {VERSION}_linux_amd64.zip\" rm \" {TOOL}_ {TOOL}_ {VERSION}_SHA256SUMS\" rm \" {TOOL}_ {TOOL}_ {VERSION} {EDITION}_SHA256SUMS\" rm \" {EDITION}_SHA256SUMS\" rm \" {TOOL}_${VERSION}_SHA256SUMS.sig\" \"${TOOL}\" --version Brew and Chocolatey repositories for Packer are usually up to date. Packer is published to **yum** and **apt-get** repositories but these installs require a substantial number of dependencies and can be quite out of date. At this time for Ubuntu 18 repository has v1.0.3 but v1.5.2 is current. ### Docker container You can also you use Packer from a container ```docker $docker pull hashicorp/packer docker images REPOSITORY TAG IMAGE ID CREATED SIZE hashicorp/packer latest 2453d5a18479 2 weeks ago 167MB You can run it much line the regular cmdline. $docker run 2453d5a18479 --version 1.35 Or $docker run -i -t hashicorp/packer:light validate You will need to pass in the Packer files by sharing the host folder into Packers Container. $docker run -v /c/code/book/packer/01-installing-packer/:/home/docker/ hashicorp/packer:light validate /home/docker/empty.json But when you run the build: $docker run -v /c/code/book/packer/01-installing-packer/:/home/docker/ hashicorp/packer:light build /home/docker/empty.json You get: Build 'docker' errored: exec: \"docker\": executable file not found in \\$PATH ==\\> Some builds didn't complete successfully and had errors: --\\> docker: exec: \"docker\": executable file not found in \\$PATH ==\\> Builds finished but no artifacts were created. Which is confusing until you realise/remember that all that's in that container is Packer and not Docker. So while you could use a Packer image to build your containers you'd have to add and customise your own anyway.","title":"Verify the SHASUM matches the binary."},{"location":"installing-packer/#packer-from-source-code","text":"You can build your own, if you are building from source, use at least version 1.12 of go-lang and follow the instructions from their readme. Assuming you have >= 1.12 golang https://github.com/hashicorp/packer/blob/master/.github/CONTRIBUTING.md#setting-up-go-to-work-on-packer Just run go get and wait. go get github.com/hashicorp/packer cd \\$gopath $ packer -version 1.35 Debugging You can run Packer in Debug mode using the -debug flag, it's fairly verbose but it is particularly useful. If you need to see which AWS call fails and/or to get the temporary SSH key that's can be used to connect to the box used to make your AMI? It'll happen.","title":"Packer from source code"},{"location":"packer-aws-ami.2/","text":"Advanced AWS authentication There are a number of more sophisticated authentication schemes for AWS. These all require an extra environmental variable - AWS_SESSION_TOKEN. AWS_ACCESS_KEY_ID=xxxxxxxxxxxx AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxx AWS_SESSION_TOKEN=xxxxxxxxxxxx The templates need an extra variable: \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\", and to the EBS builder we add: \"token\": \"{{user `aws_session_token`}}\", Assumed roles A common AWS IAM usage pattern is to create roles that can be assumed by users, either in the same AWS account or as \"cross account roles\". Assuming roles isn't yet supported directly in Packers EBS builder syntax, so for now there are two well established external methods for using assumed roles: Via scripts You can create your AWS credentials on-the-fly by calling this Powershell or a bash function and then create the environment variables to run Packer. ```powershell tab=\"Powershell\" function iam_assume_role { <# .Description iam_assume_role allows you to run as a different role in a different account .Example iam_assume_role -AccountNo $AccountNo -Role SuperAdmin > Param( [Parameter(Mandatory= true)] [string] true)] [string] AccountNo, [Parameter(Mandatory= true)] [string] true)] [string] Role ) Write-Output \"AccountNo: $AccountNo\" Write-Output \"Role : $Role\" ARN=\"arn:aws:iam:: ARN=\"arn:aws:iam:: ( AccountNo):role/ AccountNo):role/ Role\" Write-Output \"ARN : $ARN\" Write-Output \"aws sts assume-role --role-arn $ARN --role-session-name $SESSION_NAME --duration-seconds 3600\" $Creds=aws sts assume-role --role-arn $ARN --role-session-name $SESSION_NAME --duration-seconds 3600 |convertfrom-json } ```bash tab=\"Bash\" # Clear out existing AWS session environment, or the awscli call will fail unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_SECURITY_TOKEN # Old ec2 tools use other env vars unset AWS_ACCESS_KEY AWS_SECRET_KEY AWS_DELEGATION_TOKEN ROLE=\"${1:-SecurityMonkey}\" ACCOUNT=\"${2:-123456789}\" DURATION=\"${3:-900}\" NAME=\"${4:-$LOGNAME@$(hostname -s)}\" ARN=\"arn:aws:iam::${ACCOUNT}:role/$ROLE\" ECHO \"ARN: $ARN\" KST=($(aws sts assume-role --role-arn \"$ARN\" \\ --role-session-name \"$NAME\" \\ --duration-seconds \"$DURATION\" \\ --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\" \\ --output text)) echo \"export AWS_DEFAULT_REGION=\\\"eu-west-2\\\"\" echo \"export AWS_ACCESS_KEY_ID=\\\"${KST[0]}\\\"\" echo \"export AWS_ACCESS_KEY=\\\"${KST[0]}\\\"\" echo \"export AWS_SECRET_ACCESS_KEY=\\\"${KST[1]}\\\"\" echo \"export AWS_SECRET_KEY=\\\"${KST[1]}\\\"\" echo \"export AWS_SESSION_TOKEN='${KST[2]}'\" Then when you run Packer it can use the credentials of the assumed role. Via AWS-Vault AWS Vault is very handy tool for managing authentication to multiple accounts and roles. It comes from 99designs [Thanks Folks] and can found here https://github.com/99designs/aws-vault . The tool generates the same environmental variables as the scripts and also encrypts and password controls your access. This is great for local development. I've used this solution on a number of projects. Federated Authentication If your AWS account is set-up to use a federated Authentication scheme (Active Directory), your account may have no IAM users. Having no users specified makes it difficult to create access keys, and by difficult I really mean impossible. Do not then bypass your mandated security procedure and add some IAM users in. How can I work? The command line tool saml2aws https://github.com/Versent/saml2aws solves exactly this problem, it allows you to set up multiple profiles and then login to those profiles. It then updates your .aws configuration with temporary AWS credentials. You can then continue as before but either set the default profile or use named profiles. Via AWS IAM instance profile So far all the methods we have covered are aimed at how to get authentication for developing with Packer, but not for operational use. Operationally I would expect you to be running packer thru a Continuos Integration Server and build new AMIs as code changes or when the base AMIS they are made from are patched and released (Monthly). If you want to run packer in an automated fashion then the authentication needs to be provided as an IAM Instance Profile. Use the role already given but use the following to make an Instance Profile that can be used by EC2 instances. resource \"aws_iam_instance_profile\" \"packer\" { name = \"packer\" role = aws_iam_role . packer . name } You can then connect your new IAM instance profile to your EC2 instance : aws_instance.packer.tf below demonstrate how to tie it all together: resource aws_instance packer { ami = data . aws_ami . ubuntu . image_id iam_instance_profile = aws_iam_instance_profile . packer . name instance_type = var . instance_type vpc_security_group_ids = [ aws_security_group . packer . id ] associate_public_ip_address = true key_name = aws_key_pair . packer . key_name user_data = \"${file(\"${path.module}/files/userdata.sh\")}\" subnet_id = var . subnet_id tags = var . common_tags } If you provision this instance it should have the permission Packer requires.","title":"Advanced AWS authentication"},{"location":"packer-aws-ami.2/#advanced-aws-authentication","text":"There are a number of more sophisticated authentication schemes for AWS. These all require an extra environmental variable - AWS_SESSION_TOKEN. AWS_ACCESS_KEY_ID=xxxxxxxxxxxx AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxx AWS_SESSION_TOKEN=xxxxxxxxxxxx The templates need an extra variable: \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\", and to the EBS builder we add: \"token\": \"{{user `aws_session_token`}}\",","title":"Advanced AWS authentication"},{"location":"packer-aws-ami.2/#assumed-roles","text":"A common AWS IAM usage pattern is to create roles that can be assumed by users, either in the same AWS account or as \"cross account roles\". Assuming roles isn't yet supported directly in Packers EBS builder syntax, so for now there are two well established external methods for using assumed roles:","title":"Assumed roles"},{"location":"packer-aws-ami.2/#via-scripts","text":"You can create your AWS credentials on-the-fly by calling this Powershell or a bash function and then create the environment variables to run Packer. ```powershell tab=\"Powershell\" function iam_assume_role { <# .Description iam_assume_role allows you to run as a different role in a different account .Example iam_assume_role -AccountNo $AccountNo -Role SuperAdmin","title":"Via scripts"},{"location":"packer-aws-ami.2/#_1","text":"Param( [Parameter(Mandatory= true)] [string] true)] [string] AccountNo, [Parameter(Mandatory= true)] [string] true)] [string] Role ) Write-Output \"AccountNo: $AccountNo\" Write-Output \"Role : $Role\" ARN=\"arn:aws:iam:: ARN=\"arn:aws:iam:: ( AccountNo):role/ AccountNo):role/ Role\" Write-Output \"ARN : $ARN\" Write-Output \"aws sts assume-role --role-arn $ARN --role-session-name $SESSION_NAME --duration-seconds 3600\" $Creds=aws sts assume-role --role-arn $ARN --role-session-name $SESSION_NAME --duration-seconds 3600 |convertfrom-json } ```bash tab=\"Bash\" # Clear out existing AWS session environment, or the awscli call will fail unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_SECURITY_TOKEN # Old ec2 tools use other env vars unset AWS_ACCESS_KEY AWS_SECRET_KEY AWS_DELEGATION_TOKEN ROLE=\"${1:-SecurityMonkey}\" ACCOUNT=\"${2:-123456789}\" DURATION=\"${3:-900}\" NAME=\"${4:-$LOGNAME@$(hostname -s)}\" ARN=\"arn:aws:iam::${ACCOUNT}:role/$ROLE\" ECHO \"ARN: $ARN\" KST=($(aws sts assume-role --role-arn \"$ARN\" \\ --role-session-name \"$NAME\" \\ --duration-seconds \"$DURATION\" \\ --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\" \\ --output text)) echo \"export AWS_DEFAULT_REGION=\\\"eu-west-2\\\"\" echo \"export AWS_ACCESS_KEY_ID=\\\"${KST[0]}\\\"\" echo \"export AWS_ACCESS_KEY=\\\"${KST[0]}\\\"\" echo \"export AWS_SECRET_ACCESS_KEY=\\\"${KST[1]}\\\"\" echo \"export AWS_SECRET_KEY=\\\"${KST[1]}\\\"\" echo \"export AWS_SESSION_TOKEN='${KST[2]}'\" Then when you run Packer it can use the credentials of the assumed role.","title":"&gt;"},{"location":"packer-aws-ami.2/#via-aws-vault","text":"AWS Vault is very handy tool for managing authentication to multiple accounts and roles. It comes from 99designs [Thanks Folks] and can found here https://github.com/99designs/aws-vault . The tool generates the same environmental variables as the scripts and also encrypts and password controls your access. This is great for local development. I've used this solution on a number of projects.","title":"Via AWS-Vault"},{"location":"packer-aws-ami.2/#federated-authentication","text":"If your AWS account is set-up to use a federated Authentication scheme (Active Directory), your account may have no IAM users. Having no users specified makes it difficult to create access keys, and by difficult I really mean impossible. Do not then bypass your mandated security procedure and add some IAM users in. How can I work? The command line tool saml2aws https://github.com/Versent/saml2aws solves exactly this problem, it allows you to set up multiple profiles and then login to those profiles. It then updates your .aws configuration with temporary AWS credentials. You can then continue as before but either set the default profile or use named profiles.","title":"Federated Authentication"},{"location":"packer-aws-ami.2/#via-aws-iam-instance-profile","text":"So far all the methods we have covered are aimed at how to get authentication for developing with Packer, but not for operational use. Operationally I would expect you to be running packer thru a Continuos Integration Server and build new AMIs as code changes or when the base AMIS they are made from are patched and released (Monthly). If you want to run packer in an automated fashion then the authentication needs to be provided as an IAM Instance Profile. Use the role already given but use the following to make an Instance Profile that can be used by EC2 instances. resource \"aws_iam_instance_profile\" \"packer\" { name = \"packer\" role = aws_iam_role . packer . name } You can then connect your new IAM instance profile to your EC2 instance : aws_instance.packer.tf below demonstrate how to tie it all together: resource aws_instance packer { ami = data . aws_ami . ubuntu . image_id iam_instance_profile = aws_iam_instance_profile . packer . name instance_type = var . instance_type vpc_security_group_ids = [ aws_security_group . packer . id ] associate_public_ip_address = true key_name = aws_key_pair . packer . key_name user_data = \"${file(\"${path.module}/files/userdata.sh\")}\" subnet_id = var . subnet_id tags = var . common_tags } If you provision this instance it should have the permission Packer requires.","title":"Via AWS IAM instance profile"},{"location":"packer-aws-ami.3/","text":"Versioning For anything more than the basics you should have a \"Bakery\" a CI tool and process in place that regular builds new images. A Scenario where AMI's are built routinely. This creates its own set of problems. Greatest of which are \"How do I know which AMI is the best?\", \"How do build a new Imagine and Not Break something\" and \"Which of these Images has been tried and tested?\" The Images/AMIs themselves are a software Artifact and should have a development process like others do. When you build a Java library you version it, and you can do the same with Packer images if you name or tag it with a Build number. \"But I already have a git hash?\" The git hash does identify it, but it doesn't work well with Humans, QA or ordering. Thanks Git. A basic numbering Scheme You can use semantic versioning or any scheme you like, but use Semantic or AKA SemVer. Most CI tools, that you might drive your \"Bakery\" will provide/expose and/or sn environmental variable or BUILD_NUMBER that can be used. Add a variable to your template, such as BUILD_NUMBER and this will to pull the environment variable BUILD_NUMBER into Packer. \"build_number\" : \"{{env `BUILD_NUMBER`}}\" , Then modify the AMI name to reflect the new scheme: \"ami_name\" : \"RHEL-BASE-v{{user `build_number`}}-{{timestamp}}-AMI\" , With the version in the name you can now refer to it in your Terraform: data \"aws_ami\" \"rhelbase\" { most_recent = true filter { name = \"name\" values = [ \"RHEL-BASE-v${var.version}*\" ] } } You can also use the same AMI filters in your Packer Template \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"RHEL-7.6_HVM_GA-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"309956199498\" ], \"most_recent\": true }, This filter will get the latest RHEL 7.6 that's published by AWS account 309956199498 also known as Red Hat. Finding AMI'S It's much quicker to find AMI's details via the commandline. ( https://aws.amazon.com/blogs/compute/query-for-the-latest-amazon-linux-ami-ids-using-aws-systems-manager-parameter-store/ ) Via Powershell ```powershell tab=\"Powershell\" aws ec2 describe-images --filter Name=\"name\",Values=\"CentOS7*\"|convertfrom-json aws ec2 describe-images --filter Name=\"ProductCode\",Values=\"aw0evgkw8e5c1q413zgy5pjce\"|convertfrom-json ```Bash tab=\"Bash\" aws ec2 describe-images \\ --owners 679593333241 \\ --filters \\ Name=name,Values='CentOS Linux 7 x86_64 HVM EBS\\*' \\ Name=architecture,Values=x86_64 \\ Name=root-device-type,Values=ebs \\ --query 'sort_by(Images, &Name)[-1].ImageId' \\ --output text Sharing AMI's across regions Packer exposes the property ami_regions for this, however it takes an age as it copies the resulting Volumes between regions, so that'll cost you doubly. It pains me but it might be better to build the same project in each region you need it. Sharing to other accounts Again Packer has a variable for this: \"ami_users\": \"{{ user `ami_users` }}\", Which I have exposed as a variable. You can add a list of AWS account numbers to share it to. You don't need to have access to those accounts to share it with them. How to tidy up old AMI's Packer is great at making an consuming AWS resources, it's not so good at cleaning up after itself when an AMI becomes obsolete. AWS-Amicleaner from https://github.com/bonclay7/aws-amicleaner tries tp solve this issue. pip install future pip install aws-amicleaner amicleaner --version To tidy up snapshots amicleaner --check-orphans --full-report --keep-previous 20 amicleaner --mapping-key tags --mapping-values Application --full-report --keep-previous 20","title":"Versioning"},{"location":"packer-aws-ami.3/#versioning","text":"For anything more than the basics you should have a \"Bakery\" a CI tool and process in place that regular builds new images. A Scenario where AMI's are built routinely. This creates its own set of problems. Greatest of which are \"How do I know which AMI is the best?\", \"How do build a new Imagine and Not Break something\" and \"Which of these Images has been tried and tested?\" The Images/AMIs themselves are a software Artifact and should have a development process like others do. When you build a Java library you version it, and you can do the same with Packer images if you name or tag it with a Build number. \"But I already have a git hash?\" The git hash does identify it, but it doesn't work well with Humans, QA or ordering. Thanks Git.","title":"Versioning"},{"location":"packer-aws-ami.3/#a-basic-numbering-scheme","text":"You can use semantic versioning or any scheme you like, but use Semantic or AKA SemVer. Most CI tools, that you might drive your \"Bakery\" will provide/expose and/or sn environmental variable or BUILD_NUMBER that can be used. Add a variable to your template, such as BUILD_NUMBER and this will to pull the environment variable BUILD_NUMBER into Packer. \"build_number\" : \"{{env `BUILD_NUMBER`}}\" , Then modify the AMI name to reflect the new scheme: \"ami_name\" : \"RHEL-BASE-v{{user `build_number`}}-{{timestamp}}-AMI\" , With the version in the name you can now refer to it in your Terraform: data \"aws_ami\" \"rhelbase\" { most_recent = true filter { name = \"name\" values = [ \"RHEL-BASE-v${var.version}*\" ] } } You can also use the same AMI filters in your Packer Template \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"RHEL-7.6_HVM_GA-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"309956199498\" ], \"most_recent\": true }, This filter will get the latest RHEL 7.6 that's published by AWS account 309956199498 also known as Red Hat.","title":"A basic numbering Scheme"},{"location":"packer-aws-ami.3/#finding-amis","text":"It's much quicker to find AMI's details via the commandline. ( https://aws.amazon.com/blogs/compute/query-for-the-latest-amazon-linux-ami-ids-using-aws-systems-manager-parameter-store/ ) Via Powershell ```powershell tab=\"Powershell\" aws ec2 describe-images --filter Name=\"name\",Values=\"CentOS7*\"|convertfrom-json aws ec2 describe-images --filter Name=\"ProductCode\",Values=\"aw0evgkw8e5c1q413zgy5pjce\"|convertfrom-json ```Bash tab=\"Bash\" aws ec2 describe-images \\ --owners 679593333241 \\ --filters \\ Name=name,Values='CentOS Linux 7 x86_64 HVM EBS\\*' \\ Name=architecture,Values=x86_64 \\ Name=root-device-type,Values=ebs \\ --query 'sort_by(Images, &Name)[-1].ImageId' \\ --output text","title":"Finding AMI'S"},{"location":"packer-aws-ami.3/#sharing-amis-across-regions","text":"Packer exposes the property ami_regions for this, however it takes an age as it copies the resulting Volumes between regions, so that'll cost you doubly. It pains me but it might be better to build the same project in each region you need it.","title":"Sharing AMI's across regions"},{"location":"packer-aws-ami.3/#sharing-to-other-accounts","text":"Again Packer has a variable for this: \"ami_users\": \"{{ user `ami_users` }}\", Which I have exposed as a variable. You can add a list of AWS account numbers to share it to. You don't need to have access to those accounts to share it with them. How to tidy up old AMI's Packer is great at making an consuming AWS resources, it's not so good at cleaning up after itself when an AMI becomes obsolete. AWS-Amicleaner from https://github.com/bonclay7/aws-amicleaner tries tp solve this issue. pip install future pip install aws-amicleaner amicleaner --version To tidy up snapshots amicleaner --check-orphans --full-report --keep-previous 20 amicleaner --mapping-key tags --mapping-values Application --full-report --keep-previous 20","title":"Sharing to other accounts"},{"location":"packer-aws-ami/","text":"Building Amazon Machine Images (AMIs) with Packer This is most likely why you are interested in Packer, you can use Packer to help make your AMI'S, or what used to be called \"Golden Images\". The examples in this chapter target the customization of existing AWS AMI's, usually from AWS or from trusted sources such as from Canonical (Makers of Ubuntu) which are provided to your from their AWS account. You can also build your AMI's from scratch by extending the builders in Packer ISO chapter, but if theres a choice of not having to deal with answer files or installers I would, but you may want/have to. Amazon Builders There are a number builders associated with AWS: amazon-ebs amazon-chroot amazon-ebssurrogate amazon-ebsvolume amazon-instance That might sound complicated, but 99% of the time it's amazon-ebs. The first step is to set-up the Authentication. Basic AWS Authentication To use AWS with Packer you will need to set up your AWS authentication. You will need to have installed the AWS Command Line Interface CLI and obtained your credentials from the AWS console. Access Keys Click to create the keys, you will need to store these credentials safely, they are only available at creation time and if exposed can lead to significant financial loss. Running and configuring the AWS CLI Once installed, You will need to set-up and configured your AWS cli with your credentials, start by calling aws configure with the credentials you obtained previously from AWS console: aws configure Access Key ID [****************]: AWS Secret Access Key [****************]: Default region name [eu-west-1]: Default output format [json]: A simple and short test of AWS connectivity is do a list of S3 buckets: aws s3 ls It should return a list of S3 buckets, if you have made any yet that is. Using the credentials Create the file basic-auth.json { \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"\", \"secret_key\": \"\", \"region\": \"eu-west-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"099720109477\" ], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\" } ] } And try building an AMI, you should see this: $ packer build basic-auth.json amazon-ebs output will be in this color. ==> amazon-ebs: Prevalidating AMI Name... amazon-ebs: Found Image ID: ami-01bc69b830b49f729 ==> amazon-ebs: Creating temporary keypair: packer_5c94a9bd-2813-e694-e8a7-8e919e36c874 ==> amazon-ebs: Creating temporary security group for this instance: packer_5c94a9c0-710d-8fa9-cc5e-0b36ec553658 ==> amazon-ebs: Authorizing access to port 22 on the temporary security group... ==> amazon-ebs: Launching a source AWS instance... amazon-ebs: Instance ID: i-05f5aee5f989a0685 ==> amazon-ebs: Waiting for instance (i-05f5aee5f989a0685) to become ready... ==> amazon-ebs: Adding tags to source instance amazon-ebs: Adding tag: \"Name\": \"Packer Builder\" ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Stopping the source instance... amazon-ebs: Stopping instance, attempt 1 ==> amazon-ebs: Waiting for the instance to stop... ==> amazon-ebs: Creating the AMI: packer-example 1553246653 amazon-ebs: AMI: ami-069e0a0248ffbba5b ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-069e0a0248ffbba5b A number of things are assumed when you run this template, and I am fully aware that your proscribed AWS set-up and Network environment may not be so liberal and regular. You have require IAM permissions to create EBS volumes, snapshot and to create AMIS It's not best practice to use an admin or root account. In fact you should have failed in the last step due to insufficient privileges. A better way is to create just the permission you need, for a role, by creating a Packer role and inline policy in AWS. This role is describe in Terraform here as 2 files, in your terraform IAM project aws_iam_role_policy.packer.tf . resource aws_iam_role_policy packer { name = \"packer\" role = aws_iam_role . packer . id policy = <<POLICY { \"Version\": \"2012-10-17\" , \"Statement\" : [ { \"Effect\": \"Allow\" , \"Action\" : [ \"ec2:AttachVolume\" , \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:CopyImage\" , \"ec2:CreateImage\" , \"ec2:CreateKeypair\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateSnapshot\" , \"ec2:CreateTags\" , \"ec2:CreateVolume\" , \"ec2:DeleteKeyPair\" , \"ec2:DeleteSecurityGroup\" , \"ec2:DeleteSnapshot\" , \"ec2:DeleteVolume\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImageAttribute\" , \"ec2:DescribeImages\" , \"ec2:DescribeInstances\" , \"ec2:DescribeInstanceStatus\" , \"ec2:DescribeRegions\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeSnapshots\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeTags\" , \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:GetPasswordData\" , \"ec2:ModifyImageAttribute\" , \"ec2:ModifyInstanceAttribute\" , \"ec2:ModifySnapshotAttribute\" , \"ec2:RegisterImage\" , \"ec2:RunInstances\" , \"ec2:StopInstances\" , \"ec2:TerminateInstances\" ], \"Resource\" : \"*\" } ] } POLICY } And aws_iam_role.packer.tf resource aws_iam_role packer { name = \"packer\" assume_role_policy = <<POLICY { \"Version\": \"2012-10-17\" , \"Statement\" : [ { \"Action\": \"sts:AssumeRole\" , \"Principal\" : { \"Service\": \"ec2.amazonaws.com\" } , \"Effect\": \"Allow\" , \"Sid\": \"\" } ] } POLICY } You can attach the role \"packer\" to your user or group. That you are not restricted from using port 22 for ssh A not uncommon policy that some Enterprise IT administrators have, is a policy of denying ingress and egress on Port 22. Maybe not a policy I agree with, a solution is still required, fortunately you can use the packer user_data and package the change to the ssh port. The easiest way to update our packer template, is to use and specify a user_data_file , which should like like: echo 'port 2222' >> /etc/ssh/sshd_config service sshd restart Useful protection for Hackers unable to use a Port scanner. That you have still have the default Virtual Private Cloud (VPC) and Subnets set up The previous example did specify where to make the ec2 instance that is used to make the new AMI and yet somehow it worked? It used the \"Default\" VPC that comes with a new account or used the one you have specified as the default. This probably wont be the case. You can avoid hard-coding having to pass your VPC and Subnets by using filters in your template. They follow the same filter patterns as the CLI https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSubnets.html . Make basic-auth.dynamicvpc.json { \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"\", \"secret_key\": \"\", \"region\": \"eu-west-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"099720109477\" ], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\", \"vpc_filter\": { \"filters\": { \"isDefault\": \"true\" } }, \"subnet_filter\": { \"most_free\": true, \"random\": true } } ] } You will need to set up your filters to match how you have set up and named your VPC and Subnets. You have a default AWS config profile set up Packer looks for your default credentials, $HOME/.aws/credentials or %USERPROFILE%.aws. Specifying your credentials Rather than letting Packer auto-guess your credentials, it's better to be explicit and set the environmental variables Packer is expecting. These are two basic environmental variable that need setting: AWS_ACCESS_KEY_ID=xxxxxxxxx AWS_SECRET_ACCESS_KEY=xxxxxxxxx You can check what you have set with: Linux and Mac ```bash tab=\"*nix\" printenv ```cmd tab=\"CMD\" set ```powershell tab=\"Powershell\" Get-ChildItem Env: With Use Export, set \\$env: to set your variables, you can temporarily set environmental variables (\\*nix, CMD or Powershell): ```cli export AWS_ACCESS_KEY_ID=xxxxx set AWS_ACCESS_KEY_ID=xxxxx $env:AWS_ACCESS_KEY_ID = xxxxx Now try building an AMI again but with the credentials pulled from the environmental variables: $ packer build basic-auth.withcreds.json ... { \"variables\": { \"aws_access_key\": \"{{env `aws_access_key`}}\", \"aws_secret_key\": \"{{env `aws_secret_key`}}\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\", \"region\": \"eu-west-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"099720109477\" ], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\" } ] } Time to try this all out again: $ packer build basic-auth.withcreds.json So if you only have one account this might seem like a lot of extra cruft. In other situations it's a life saver.","title":"AWS AMI"},{"location":"packer-aws-ami/#building-amazon-machine-images-amis-with-packer","text":"This is most likely why you are interested in Packer, you can use Packer to help make your AMI'S, or what used to be called \"Golden Images\". The examples in this chapter target the customization of existing AWS AMI's, usually from AWS or from trusted sources such as from Canonical (Makers of Ubuntu) which are provided to your from their AWS account. You can also build your AMI's from scratch by extending the builders in Packer ISO chapter, but if theres a choice of not having to deal with answer files or installers I would, but you may want/have to.","title":"Building Amazon Machine Images (AMIs) with Packer"},{"location":"packer-aws-ami/#amazon-builders","text":"There are a number builders associated with AWS: amazon-ebs amazon-chroot amazon-ebssurrogate amazon-ebsvolume amazon-instance That might sound complicated, but 99% of the time it's amazon-ebs. The first step is to set-up the Authentication.","title":"Amazon Builders"},{"location":"packer-aws-ami/#basic-aws-authentication","text":"To use AWS with Packer you will need to set up your AWS authentication. You will need to have installed the AWS Command Line Interface CLI and obtained your credentials from the AWS console. Access Keys Click to create the keys, you will need to store these credentials safely, they are only available at creation time and if exposed can lead to significant financial loss.","title":"Basic AWS Authentication"},{"location":"packer-aws-ami/#running-and-configuring-the-aws-cli","text":"Once installed, You will need to set-up and configured your AWS cli with your credentials, start by calling aws configure with the credentials you obtained previously from AWS console: aws configure Access Key ID [****************]: AWS Secret Access Key [****************]: Default region name [eu-west-1]: Default output format [json]: A simple and short test of AWS connectivity is do a list of S3 buckets: aws s3 ls It should return a list of S3 buckets, if you have made any yet that is.","title":"Running and configuring the AWS CLI"},{"location":"packer-aws-ami/#using-the-credentials","text":"Create the file basic-auth.json { \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"\", \"secret_key\": \"\", \"region\": \"eu-west-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"099720109477\" ], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\" } ] } And try building an AMI, you should see this: $ packer build basic-auth.json amazon-ebs output will be in this color. ==> amazon-ebs: Prevalidating AMI Name... amazon-ebs: Found Image ID: ami-01bc69b830b49f729 ==> amazon-ebs: Creating temporary keypair: packer_5c94a9bd-2813-e694-e8a7-8e919e36c874 ==> amazon-ebs: Creating temporary security group for this instance: packer_5c94a9c0-710d-8fa9-cc5e-0b36ec553658 ==> amazon-ebs: Authorizing access to port 22 on the temporary security group... ==> amazon-ebs: Launching a source AWS instance... amazon-ebs: Instance ID: i-05f5aee5f989a0685 ==> amazon-ebs: Waiting for instance (i-05f5aee5f989a0685) to become ready... ==> amazon-ebs: Adding tags to source instance amazon-ebs: Adding tag: \"Name\": \"Packer Builder\" ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Stopping the source instance... amazon-ebs: Stopping instance, attempt 1 ==> amazon-ebs: Waiting for the instance to stop... ==> amazon-ebs: Creating the AMI: packer-example 1553246653 amazon-ebs: AMI: ami-069e0a0248ffbba5b ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-069e0a0248ffbba5b A number of things are assumed when you run this template, and I am fully aware that your proscribed AWS set-up and Network environment may not be so liberal and regular.","title":"Using the credentials"},{"location":"packer-aws-ami/#you-have-require-iam-permissions-to-create-ebs-volumes-snapshot-and-to-create-amis","text":"It's not best practice to use an admin or root account. In fact you should have failed in the last step due to insufficient privileges. A better way is to create just the permission you need, for a role, by creating a Packer role and inline policy in AWS. This role is describe in Terraform here as 2 files, in your terraform IAM project aws_iam_role_policy.packer.tf . resource aws_iam_role_policy packer { name = \"packer\" role = aws_iam_role . packer . id policy = <<POLICY { \"Version\": \"2012-10-17\" , \"Statement\" : [ { \"Effect\": \"Allow\" , \"Action\" : [ \"ec2:AttachVolume\" , \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:CopyImage\" , \"ec2:CreateImage\" , \"ec2:CreateKeypair\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateSnapshot\" , \"ec2:CreateTags\" , \"ec2:CreateVolume\" , \"ec2:DeleteKeyPair\" , \"ec2:DeleteSecurityGroup\" , \"ec2:DeleteSnapshot\" , \"ec2:DeleteVolume\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImageAttribute\" , \"ec2:DescribeImages\" , \"ec2:DescribeInstances\" , \"ec2:DescribeInstanceStatus\" , \"ec2:DescribeRegions\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeSnapshots\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeTags\" , \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:GetPasswordData\" , \"ec2:ModifyImageAttribute\" , \"ec2:ModifyInstanceAttribute\" , \"ec2:ModifySnapshotAttribute\" , \"ec2:RegisterImage\" , \"ec2:RunInstances\" , \"ec2:StopInstances\" , \"ec2:TerminateInstances\" ], \"Resource\" : \"*\" } ] } POLICY } And aws_iam_role.packer.tf resource aws_iam_role packer { name = \"packer\" assume_role_policy = <<POLICY { \"Version\": \"2012-10-17\" , \"Statement\" : [ { \"Action\": \"sts:AssumeRole\" , \"Principal\" : { \"Service\": \"ec2.amazonaws.com\" } , \"Effect\": \"Allow\" , \"Sid\": \"\" } ] } POLICY } You can attach the role \"packer\" to your user or group.","title":"You have require IAM permissions to create EBS volumes, snapshot and to create AMIS"},{"location":"packer-aws-ami/#that-you-are-not-restricted-from-using-port-22-for-ssh","text":"A not uncommon policy that some Enterprise IT administrators have, is a policy of denying ingress and egress on Port 22. Maybe not a policy I agree with, a solution is still required, fortunately you can use the packer user_data and package the change to the ssh port. The easiest way to update our packer template, is to use and specify a user_data_file , which should like like: echo 'port 2222' >> /etc/ssh/sshd_config service sshd restart Useful protection for Hackers unable to use a Port scanner.","title":"That you are not restricted from using port 22 for ssh"},{"location":"packer-aws-ami/#that-you-have-still-have-the-default-virtual-private-cloud-vpc-and-subnets-set-up","text":"The previous example did specify where to make the ec2 instance that is used to make the new AMI and yet somehow it worked? It used the \"Default\" VPC that comes with a new account or used the one you have specified as the default. This probably wont be the case. You can avoid hard-coding having to pass your VPC and Subnets by using filters in your template. They follow the same filter patterns as the CLI https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSubnets.html . Make basic-auth.dynamicvpc.json { \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"\", \"secret_key\": \"\", \"region\": \"eu-west-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"099720109477\" ], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\", \"vpc_filter\": { \"filters\": { \"isDefault\": \"true\" } }, \"subnet_filter\": { \"most_free\": true, \"random\": true } } ] } You will need to set up your filters to match how you have set up and named your VPC and Subnets.","title":"That you have still have the default Virtual Private Cloud (VPC) and Subnets set up"},{"location":"packer-aws-ami/#you-have-a-default-aws-config-profile-set-up","text":"Packer looks for your default credentials, $HOME/.aws/credentials or %USERPROFILE%.aws.","title":"You have a default AWS config profile set up"},{"location":"packer-aws-ami/#specifying-your-credentials","text":"Rather than letting Packer auto-guess your credentials, it's better to be explicit and set the environmental variables Packer is expecting. These are two basic environmental variable that need setting: AWS_ACCESS_KEY_ID=xxxxxxxxx AWS_SECRET_ACCESS_KEY=xxxxxxxxx You can check what you have set with: Linux and Mac ```bash tab=\"*nix\" printenv ```cmd tab=\"CMD\" set ```powershell tab=\"Powershell\" Get-ChildItem Env: With Use Export, set \\$env: to set your variables, you can temporarily set environmental variables (\\*nix, CMD or Powershell): ```cli export AWS_ACCESS_KEY_ID=xxxxx set AWS_ACCESS_KEY_ID=xxxxx $env:AWS_ACCESS_KEY_ID = xxxxx Now try building an AMI again but with the credentials pulled from the environmental variables: $ packer build basic-auth.withcreds.json ... { \"variables\": { \"aws_access_key\": \"{{env `aws_access_key`}}\", \"aws_secret_key\": \"{{env `aws_secret_key`}}\" }, \"builders\": [ { \"type\": \"amazon-ebs\", \"access_key\": \"{{user `aws_access_key`}}\", \"secret_key\": \"{{user `aws_secret_key`}}\", \"region\": \"eu-west-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"099720109477\" ], \"most_recent\": true }, \"instance_type\": \"t2.micro\", \"ssh_username\": \"ubuntu\", \"ami_name\": \"packer-example {{timestamp}}\" } ] } Time to try this all out again: $ packer build basic-auth.withcreds.json So if you only have one account this might seem like a lot of extra cruft. In other situations it's a life saver.","title":"Specifying your credentials"},{"location":"packer-file/","text":"File Header Files. Well apart from debugging this one's lost on me. So for completeness here's as example using the file builder in the file packer-file.json : { \"builders\" : [ { \"type\" : \"file\" , \"content\" : \"# File Header \\n multi-line also \\n The End\" , \"target\" : \"output.md\" } ] } Can't say I learned. packer build . \\p acker-file.json file output will be in this color. Build 'file' finished. == > Builds finished. The artifacts of successful builds are: --> file: Stored file: output.md Not much more to see.","title":"File"},{"location":"packer-file/#file-header","text":"Files. Well apart from debugging this one's lost on me. So for completeness here's as example using the file builder in the file packer-file.json : { \"builders\" : [ { \"type\" : \"file\" , \"content\" : \"# File Header \\n multi-line also \\n The End\" , \"target\" : \"output.md\" } ] } Can't say I learned. packer build . \\p acker-file.json file output will be in this color. Build 'file' finished. == > Builds finished. The artifacts of successful builds are: --> file: Stored file: output.md Not much more to see.","title":"File Header"},{"location":"packer-iso/","text":"Building an ISO image with Packer AVOID OK Building ISOs takes time and patience and is super fiddly. There's much better ways to spend your time. You can download these prebuilt from Canonical. cloud-images.ubuntu.com You can make ISO/disc images for Virtual machine platforms using a number of Packer builders, I'm going to give a number of examples for VirtualBox and Ubuntu:- Virtualbox ISO The first example is to build a very basic Ubuntu vbox. virtualbox-ubuntu.json . { \"builders\" : [ { \"type\" : \"virtualbox-iso\" , \"guest_os_type\" : \"Ubuntu_64\" , \"iso_url\" : \"\" , \"iso_checksum\" : \"\" , \"iso_checksum_type\" : \"sha256\" , \"ssh_username\" : \"packer\" , \"ssh_password\" : \"packer\" , \"shutdown_command\" : \"echo 'packer' | sudo -S shutdown -P now\" } ] } This is far from complete, the first thing we need to add is what Ubunt ISO to load, and then to also provide the checksums, that's SHA256 these days. Ubuntu 16.04 can be found from its release location - http://releases.ubuntu.com/16.04/ubuntu-16.04.6-server-amd64.iso . And checksum - the SHA256 can be retrieved from http://releases.ubuntu.com/16.04/SHA256SUMS 16afb1375372c57471ea5e29803a89a5a6bd1f6aabea2e5e34ac1ab7eb9786ac Update your packer template, it should now look like this virtualbox-ubuntu.json . { \"builders\" : [ { \"type\" : \"virtualbox-iso\" , \"guest_os_type\" : \"Ubuntu_64\" , \"iso_url\" : \"http://releases.ubuntu.com/16.04/ubuntu-16.04.6-server-amd64.iso\" , \"iso_checksum\" : \"16afb1375372c57471ea5e29803a89a5a6bd1f6aabea2e5e34ac1ab7eb9786ac\" , \"iso_checksum_type\" : \"sha256\" , \"ssh_username\" : \"packer\" , \"ssh_password\" : \"packer\" , \"shutdown_command\" : \"echo 'packer' | sudo -S shutdown -P now\" } ] } Time to try it out. packer build virtualbox-ubuntu.json Depending on your available bandwidth, it's going to run a while as it down loads the ISO. virtualbox-iso output will be in this color. ==> virtualbox-iso: Retrieving Guest additions virtualbox-iso: Using file in-place: file:///C:/Program%20Files/Oracle/VirtualBox/VBoxGuestAdditions.iso ==> virtualbox-iso: Retrieving ISO 1 items: 873.00 MiB / 873.00 MiB 4m2s virtualbox-iso: Transferred: http://releases.ubuntu.com/16.04/ubuntu-16.04.6-server-amd64.iso ==> virtualbox-iso: Creating virtual machine... ==> virtualbox-iso: Creating hard drive... ==> virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 2749) ==> virtualbox-iso: Starting the virtual machine... ==> virtualbox-iso: Waiting 10s for boot... ==> virtualbox-iso: Typing the boot command... ==> virtualbox-iso: Using ssh communicator to connect: 127.0.0.1 ==> virtualbox-iso: Waiting for SSH to become available... Yes I ran it from a Windows 10 PC. A smart move would be to save these ISO locally, and use them from there. So the output has halted but you should see that Virtual box is running, should look a bit like this: It would be unfair at this point if I didnt indicate that the process was yet fully automated, running packer now would boot your VM but leave it waiting for you to answer and supply values to the installer. What's the point of that, not much DevOps there? The crucial missing part of the puzzle, is the boot_command and the preseed file. These are the instructions provided to the VM that answer or configure the machine image, well for this builder and OS it's: \"boot_command\" : [ \"\" , \"\" , \"\" , \"/install/vmlinuz\" , \" auto\" , \" console-setup/ask_detect=false\" , \" console-setup/layoutcode=us\" , \" console-setup/modelcode=pc105\" , \" debconf/frontend=noninteractive\" , \" debian-installer=en_US\" , \" fb=false\" , \" initrd=/install/initrd.gz\" , \" kbd-chooser/method=us\" , \" keyboard-configuration/layout=USA\" , \" keyboard-configuration/variant=USA\" , \" locale=en_US\" , \" netcfg/get_domain=vm\" , \" netcfg/get_hostname=vagrant\" , \" grub-installer/bootdev=/dev/sda\" , \" noapic\" , \" preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg\" , \" -- \" , \"\" ] , The Boot command is composed of a number of different pieces, here it is: https://www.packer.io/docs/builders/virtualbox-iso.html It may take some significant effort to get the boot_command correct, you may find a significant number of the samples on the internet do not work. Pre-seed file In the boot_command was a reference: \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \" , Validate your new template: packer validate virtualbox-ubuntu.json Template validated successfully. That's all working fine, we can try building now. packer build virtualbox-ubuntu.json.json Troubleshooting the virtual box builder Running virtual-box on Windows: If you get errors of the type \"VirtualBox Won't Run - raw-mode unavailable courtesy of Hyper-V\" then you need to run this command at your console as admin: bcdedit /set hypervisorlaunchtype off If your running Hyperv for other reasons (i.e. Docker) you will need to revert this later: bcdedit /set hypervisorlaunchtype auto Ubuntu specific Fails to reboot and waits There are at 2 layers of automation ended into the ubuntu installer. Debian - d-i and tghe preseed.ccg file. Ubiquity from Ubuntu To get it to reboot add this to your preseed.cfg file: # dont prompt d-i finish-install/reboot_in_progress note ubiquity ubiquity/summary note ubiquity ubiquity/reboot boolean true and this to your packer boot_command \" automatic-ubiquity<wait>\" , \" ubiquity/reboot=true<wait>\" , \"sudo: sorry, you must have a tty to run sudo\" This was easier to fix, add this line to packer virtualbox \"ssh_pty\" : \"true\" , Also adding a script Provisioner { \"type\" : \"shell\" , \"execute_command\" : \"echo '{{user `ssh_pass`}}' | {{ .Vars }} sudo -E -S sh '{{ .Path }}'\" , \"inline\" : [ \"echo '%sudo ALL=(ALL) NOPASSWD:ALL'>> /etc/sudoers\" ] }","title":"ISO"},{"location":"packer-iso/#building-an-iso-image-with-packer","text":"AVOID OK Building ISOs takes time and patience and is super fiddly. There's much better ways to spend your time. You can download these prebuilt from Canonical. cloud-images.ubuntu.com You can make ISO/disc images for Virtual machine platforms using a number of Packer builders, I'm going to give a number of examples for VirtualBox and Ubuntu:-","title":"Building an ISO image with Packer"},{"location":"packer-iso/#virtualbox-iso","text":"The first example is to build a very basic Ubuntu vbox. virtualbox-ubuntu.json . { \"builders\" : [ { \"type\" : \"virtualbox-iso\" , \"guest_os_type\" : \"Ubuntu_64\" , \"iso_url\" : \"\" , \"iso_checksum\" : \"\" , \"iso_checksum_type\" : \"sha256\" , \"ssh_username\" : \"packer\" , \"ssh_password\" : \"packer\" , \"shutdown_command\" : \"echo 'packer' | sudo -S shutdown -P now\" } ] } This is far from complete, the first thing we need to add is what Ubunt ISO to load, and then to also provide the checksums, that's SHA256 these days. Ubuntu 16.04 can be found from its release location - http://releases.ubuntu.com/16.04/ubuntu-16.04.6-server-amd64.iso . And checksum - the SHA256 can be retrieved from http://releases.ubuntu.com/16.04/SHA256SUMS 16afb1375372c57471ea5e29803a89a5a6bd1f6aabea2e5e34ac1ab7eb9786ac Update your packer template, it should now look like this virtualbox-ubuntu.json . { \"builders\" : [ { \"type\" : \"virtualbox-iso\" , \"guest_os_type\" : \"Ubuntu_64\" , \"iso_url\" : \"http://releases.ubuntu.com/16.04/ubuntu-16.04.6-server-amd64.iso\" , \"iso_checksum\" : \"16afb1375372c57471ea5e29803a89a5a6bd1f6aabea2e5e34ac1ab7eb9786ac\" , \"iso_checksum_type\" : \"sha256\" , \"ssh_username\" : \"packer\" , \"ssh_password\" : \"packer\" , \"shutdown_command\" : \"echo 'packer' | sudo -S shutdown -P now\" } ] } Time to try it out. packer build virtualbox-ubuntu.json Depending on your available bandwidth, it's going to run a while as it down loads the ISO. virtualbox-iso output will be in this color. ==> virtualbox-iso: Retrieving Guest additions virtualbox-iso: Using file in-place: file:///C:/Program%20Files/Oracle/VirtualBox/VBoxGuestAdditions.iso ==> virtualbox-iso: Retrieving ISO 1 items: 873.00 MiB / 873.00 MiB 4m2s virtualbox-iso: Transferred: http://releases.ubuntu.com/16.04/ubuntu-16.04.6-server-amd64.iso ==> virtualbox-iso: Creating virtual machine... ==> virtualbox-iso: Creating hard drive... ==> virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 2749) ==> virtualbox-iso: Starting the virtual machine... ==> virtualbox-iso: Waiting 10s for boot... ==> virtualbox-iso: Typing the boot command... ==> virtualbox-iso: Using ssh communicator to connect: 127.0.0.1 ==> virtualbox-iso: Waiting for SSH to become available... Yes I ran it from a Windows 10 PC. A smart move would be to save these ISO locally, and use them from there. So the output has halted but you should see that Virtual box is running, should look a bit like this: It would be unfair at this point if I didnt indicate that the process was yet fully automated, running packer now would boot your VM but leave it waiting for you to answer and supply values to the installer. What's the point of that, not much DevOps there? The crucial missing part of the puzzle, is the boot_command and the preseed file. These are the instructions provided to the VM that answer or configure the machine image, well for this builder and OS it's: \"boot_command\" : [ \"\" , \"\" , \"\" , \"/install/vmlinuz\" , \" auto\" , \" console-setup/ask_detect=false\" , \" console-setup/layoutcode=us\" , \" console-setup/modelcode=pc105\" , \" debconf/frontend=noninteractive\" , \" debian-installer=en_US\" , \" fb=false\" , \" initrd=/install/initrd.gz\" , \" kbd-chooser/method=us\" , \" keyboard-configuration/layout=USA\" , \" keyboard-configuration/variant=USA\" , \" locale=en_US\" , \" netcfg/get_domain=vm\" , \" netcfg/get_hostname=vagrant\" , \" grub-installer/bootdev=/dev/sda\" , \" noapic\" , \" preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg\" , \" -- \" , \"\" ] , The Boot command is composed of a number of different pieces, here it is: https://www.packer.io/docs/builders/virtualbox-iso.html It may take some significant effort to get the boot_command correct, you may find a significant number of the samples on the internet do not work.","title":"Virtualbox ISO"},{"location":"packer-iso/#pre-seed-file","text":"In the boot_command was a reference: \"preseed/url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg \" , Validate your new template: packer validate virtualbox-ubuntu.json Template validated successfully. That's all working fine, we can try building now. packer build virtualbox-ubuntu.json.json","title":"Pre-seed file"},{"location":"packer-iso/#troubleshooting-the-virtual-box-builder","text":"Running virtual-box on Windows: If you get errors of the type \"VirtualBox Won't Run - raw-mode unavailable courtesy of Hyper-V\" then you need to run this command at your console as admin: bcdedit /set hypervisorlaunchtype off If your running Hyperv for other reasons (i.e. Docker) you will need to revert this later: bcdedit /set hypervisorlaunchtype auto","title":"Troubleshooting the virtual box builder"},{"location":"packer-iso/#ubuntu-specific","text":"","title":"Ubuntu specific"},{"location":"packer-iso/#fails-to-reboot-and-waits","text":"There are at 2 layers of automation ended into the ubuntu installer. Debian - d-i and tghe preseed.ccg file. Ubiquity from Ubuntu To get it to reboot add this to your preseed.cfg file: # dont prompt d-i finish-install/reboot_in_progress note ubiquity ubiquity/summary note ubiquity ubiquity/reboot boolean true and this to your packer boot_command \" automatic-ubiquity<wait>\" , \" ubiquity/reboot=true<wait>\" ,","title":"Fails to reboot and waits"},{"location":"packer-iso/#sudo-sorry-you-must-have-a-tty-to-run-sudo","text":"This was easier to fix, add this line to packer virtualbox \"ssh_pty\" : \"true\" , Also adding a script Provisioner { \"type\" : \"shell\" , \"execute_command\" : \"echo '{{user `ssh_pass`}}' | {{ .Vars }} sudo -E -S sh '{{ .Path }}'\" , \"inline\" : [ \"echo '%sudo ALL=(ALL) NOPASSWD:ALL'>> /etc/sudoers\" ] }","title":"\"sudo: sorry, you must have a tty to run sudo\""},{"location":"packer-null/","text":"Null builders This builder does Null or actually nothing. What's the use in that? Well sometimes you'll want to test some other part of a provisioner or post-processor and not wait 10 minutes for a build to run. It also doesn't cost anything to run. packer-null.json is the minimal configuration possible. { \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ] } Then if you run it: null output will be in this color. Build 'null' finished. ==> Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder Hopefully I set your expectations low, so no suprises. A null communicator is not that helpful, but you can set a few other parameters. You can set the builder to talk to an existing instance: \"ssh_username\": \"scott\", \"ssh_password\": \"tiger\", \"ssh_host\": \"10.0.0.1\" So you can now imagine there are some scenarios where it does make some sense after all, as it will save you time and money (especially if making AMIs). Examples with Provisioners File Try making and running packer-null-file.json { \"builders\": [ { \"type\": \"null\", \"ssh_username\": \"pogo\", \"ssh_password\": \"BigYellowHair\", \"ssh_host\": \"192.168.1.139\" } ], \"provisioners\": [ { \"type\": \"file\", \"source\": \"hello-world.sh\", \"destination\": \"hello-world.sh\" } ] } This should give you: ```shell null output will be in this color. ==> null: Using ssh communicator to connect: 192.168.1.139 ==> null: Waiting for SSH to become available... ==> null: Connected to SSH! ==> null: Uploading hello-world.sh => hello-world.sh 1 items: 30 B / 30 B [=============================================================================================] 0s Build 'null' finished. ==> Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder Shell-local Try making and running packer-null-shell-local.json { \"builders\": [ { \"type\": \"null\", \"ssh_username\": \"sshd\", \"ssh_password\": \"HAx0rDude5\", \"ssh_host\": \"192.168.1.12\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": \"echo hello world\" } ] } Running this simple example you get: $ packer build packer-null-script.1.json null output will be in this color. == > null: Using ssh communicator to connect: 192 .168.1.139 == > null: Waiting for SSH to become available... == > null: Connected to SSH! == > null: Running local shell script: /tmp/packer-shell769974643 null: hello world Build 'null' finished. == > Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder So as you can see the NULL provisioner is a useful development and testing tool to aid packer template development. File builder Another odd one for the development pile, allows you to test post-processors quickly. { \"builders\" : [ { \"type\" : \"file\" , \"content\" : \"# File Header \\n multi-line also \\n The End\" , \"target\" : \"README.md\" } ] }","title":"Null builders"},{"location":"packer-null/#null-builders","text":"This builder does Null or actually nothing. What's the use in that? Well sometimes you'll want to test some other part of a provisioner or post-processor and not wait 10 minutes for a build to run. It also doesn't cost anything to run. packer-null.json is the minimal configuration possible. { \"builders\": [ { \"type\": \"null\", \"communicator\": \"none\" } ] } Then if you run it: null output will be in this color. Build 'null' finished. ==> Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder Hopefully I set your expectations low, so no suprises. A null communicator is not that helpful, but you can set a few other parameters. You can set the builder to talk to an existing instance: \"ssh_username\": \"scott\", \"ssh_password\": \"tiger\", \"ssh_host\": \"10.0.0.1\" So you can now imagine there are some scenarios where it does make some sense after all, as it will save you time and money (especially if making AMIs).","title":"Null builders"},{"location":"packer-null/#examples-with-provisioners","text":"","title":"Examples with Provisioners"},{"location":"packer-null/#file","text":"Try making and running packer-null-file.json { \"builders\": [ { \"type\": \"null\", \"ssh_username\": \"pogo\", \"ssh_password\": \"BigYellowHair\", \"ssh_host\": \"192.168.1.139\" } ], \"provisioners\": [ { \"type\": \"file\", \"source\": \"hello-world.sh\", \"destination\": \"hello-world.sh\" } ] } This should give you: ```shell null output will be in this color. ==> null: Using ssh communicator to connect: 192.168.1.139 ==> null: Waiting for SSH to become available... ==> null: Connected to SSH! ==> null: Uploading hello-world.sh => hello-world.sh 1 items: 30 B / 30 B [=============================================================================================] 0s Build 'null' finished. ==> Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder","title":"File"},{"location":"packer-null/#shell-local","text":"Try making and running packer-null-shell-local.json { \"builders\": [ { \"type\": \"null\", \"ssh_username\": \"sshd\", \"ssh_password\": \"HAx0rDude5\", \"ssh_host\": \"192.168.1.12\" } ], \"provisioners\": [ { \"type\": \"shell-local\", \"inline\": \"echo hello world\" } ] } Running this simple example you get: $ packer build packer-null-script.1.json null output will be in this color. == > null: Using ssh communicator to connect: 192 .168.1.139 == > null: Waiting for SSH to become available... == > null: Connected to SSH! == > null: Running local shell script: /tmp/packer-shell769974643 null: hello world Build 'null' finished. == > Builds finished. The artifacts of successful builds are: --> null: Did not export anything. This is the null builder So as you can see the NULL provisioner is a useful development and testing tool to aid packer template development.","title":"Shell-local"},{"location":"packer-null/#file-builder","text":"Another odd one for the development pile, allows you to test post-processors quickly. { \"builders\" : [ { \"type\" : \"file\" , \"content\" : \"# File Header \\n multi-line also \\n The End\" , \"target\" : \"README.md\" } ] }","title":"File builder"},{"location":"post-processor/","text":"Post-processor You can add a post-processor section to your Packer templates. In this case add a Vagrant to an Ubuntu AMI build \"post-processors\" : [ \"vagrant\" ] So you end with base-aws.tovagrant.json { \"variables\" : { \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"aws_session_token\" : \"{{env `AWS_SESSION_TOKEN`}}\" , \"build_number\" : \"{{env `BUILD_NUMBER`}}\" , \"aws-region\" : \"{{env `AWS_REGION`}}\" , \"instance_type\" : \"t2.micro\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"access_key\" : \"{{user `aws_access_key`}}\" , \"secret_key\" : \"{{user `aws_secret_key`}}\" , \"token\" : \"{{user `aws_session_token`}}\" , \"region\" : \"{{user `aws_region`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"RHEL-7.6_HVM_GA-*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"309956199498\" ], \"most_recent\" : true }, \"instance_type\" : \"{{ user `instance_type` }}\" , \"ssh_username\" : \"ec2-user\" , \"ami_name\" : \"RHEL-BASE-v{{user `build_number`}}-{{timestamp}}-AMI\" , \"ami_description\" : \"RHEL base AMI\" , \"ami_virtualization_type\" : \"hvm\" , \"ami_users\" : \"{{ user `ami_users` }}\" , \"temporary_key_pair_name\" : \"rhel-packer-{{timestamp}}\" , \"vpc_id\" : \"{{user `vpc_id`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" , \"associate_public_ip_address\" : true , \"run_tags\" : { \"Name\" : \"rhel-base-packer\" , \"Application\" : \"base\" }, \"tags\" : { \"OS_Version\" : \"RedHat7\" , \"Version\" : \"{{user `build_number`}}\" , \"Application\" : \"Base Image\" , \"Runner\" : \"EC2\" } } ], \"post-processors\" : [ \"vagrant\" ] } When you run this template with this section added: packer build .\\base-aws.tovagrant.json amazon-ebs output will be in this color. ==> amazon-ebs: Prevalidating AMI Name: RHEL-BASE-v-1553515786-AMI amazon-ebs: Found Image ID: ami-0202869bdd0fc8c75 ==> amazon-ebs: Creating temporary keypair: rhel-packer-1553515786 ==> amazon-ebs: Creating temporary security group for this instance: packer_5c98c50d-5cea-f8fa-e4f9-190bbb536434 ==> amazon-ebs: Authorizing access to port 22 from 0.0.0.0/0 in the temporary security group... ==> amazon-ebs: Launching a source AWS instance... ==> amazon-ebs: Adding tags to source instance amazon-ebs: Adding tag: \"Application\": \"base\" amazon-ebs: Adding tag: \"Name\": \"rhel-base-packer\" amazon-ebs: Instance ID: i-03777753ff9da5a90 ==> amazon-ebs: Waiting for instance (i-03777753ff9da5a90) to become ready... ==> amazon-ebs: Using ssh communicator to connect: 54.229.138.43 ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Stopping the source instance... amazon-ebs: Stopping instance, attempt 1 ==> amazon-ebs: Waiting for the instance to stop... ==> amazon-ebs: Creating unencrypted AMI RHEL-BASE-v-1553515786-AMI from instance i-03777753ff9da5a90 amazon-ebs: AMI: ami-09e36b5de41d52201 ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Modifying attributes on AMI (ami-09e36b5de41d52201)... amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on snapshot (snap-03c17a67bf38f946f)... ==> amazon-ebs: Adding tags to AMI (ami-09e36b5de41d52201)... ==> amazon-ebs: Tagging snapshot: snap-03c17a67bf38f946f ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"Version\": \"\" amazon-ebs: Adding tag: \"Application\": \"Base Image\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"OS_Version\": \"RedHat7\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... ==> amazon-ebs: Running post-processor: vagrant ==> amazon-ebs (vagrant): Creating Vagrant box for 'aws' provider amazon-ebs (vagrant): Compressing: Vagrantfile amazon-ebs (vagrant): Compressing: metadata.json Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-09e36b5de41d52201 --> amazon-ebs: 'aws' provider box: packer_amazon-ebs_aws.box You get your regular AMI made plus you'll notice that the last few lines are different, instead of the usual build and output it leaves you you with a binary file packer_amazon-ebs_aws.box . This can be added to your Vagrant with: vagrant box add fancy-box packer_amazon-ebs_aws.box vagrant box add fancy -box packer_amazon-ebs_aws.box ==> box: Box file was not detected as metadata. Adding it directly... ==> box: Adding box 'fancy-box' (v0) for provider: box: Unpacking necessary files from: file://C:/code/book/Image-Creation-using-Packer/packer-ami/amazon-ebs/linux/packer_amazon-ebs_aws.box box: Progress: 100% (Rate: 349k/s, Estimated time remaining: --:--:--) ==> box: Successfully added box 'fancy-box' (v0) for 'aws'! Amazing!","title":"Post-processor"},{"location":"post-processor/#post-processor","text":"You can add a post-processor section to your Packer templates. In this case add a Vagrant to an Ubuntu AMI build \"post-processors\" : [ \"vagrant\" ] So you end with base-aws.tovagrant.json { \"variables\" : { \"aws_access_key\" : \"{{env `AWS_ACCESS_KEY_ID`}}\" , \"aws_secret_key\" : \"{{env `AWS_SECRET_ACCESS_KEY`}}\" , \"aws_session_token\" : \"{{env `AWS_SESSION_TOKEN`}}\" , \"build_number\" : \"{{env `BUILD_NUMBER`}}\" , \"aws-region\" : \"{{env `AWS_REGION`}}\" , \"instance_type\" : \"t2.micro\" }, \"builders\" : [ { \"type\" : \"amazon-ebs\" , \"access_key\" : \"{{user `aws_access_key`}}\" , \"secret_key\" : \"{{user `aws_secret_key`}}\" , \"token\" : \"{{user `aws_session_token`}}\" , \"region\" : \"{{user `aws_region`}}\" , \"source_ami_filter\" : { \"filters\" : { \"virtualization-type\" : \"hvm\" , \"name\" : \"RHEL-7.6_HVM_GA-*\" , \"root-device-type\" : \"ebs\" }, \"owners\" : [ \"309956199498\" ], \"most_recent\" : true }, \"instance_type\" : \"{{ user `instance_type` }}\" , \"ssh_username\" : \"ec2-user\" , \"ami_name\" : \"RHEL-BASE-v{{user `build_number`}}-{{timestamp}}-AMI\" , \"ami_description\" : \"RHEL base AMI\" , \"ami_virtualization_type\" : \"hvm\" , \"ami_users\" : \"{{ user `ami_users` }}\" , \"temporary_key_pair_name\" : \"rhel-packer-{{timestamp}}\" , \"vpc_id\" : \"{{user `vpc_id`}}\" , \"subnet_id\" : \"{{user `subnet_id`}}\" , \"associate_public_ip_address\" : true , \"run_tags\" : { \"Name\" : \"rhel-base-packer\" , \"Application\" : \"base\" }, \"tags\" : { \"OS_Version\" : \"RedHat7\" , \"Version\" : \"{{user `build_number`}}\" , \"Application\" : \"Base Image\" , \"Runner\" : \"EC2\" } } ], \"post-processors\" : [ \"vagrant\" ] } When you run this template with this section added: packer build .\\base-aws.tovagrant.json amazon-ebs output will be in this color. ==> amazon-ebs: Prevalidating AMI Name: RHEL-BASE-v-1553515786-AMI amazon-ebs: Found Image ID: ami-0202869bdd0fc8c75 ==> amazon-ebs: Creating temporary keypair: rhel-packer-1553515786 ==> amazon-ebs: Creating temporary security group for this instance: packer_5c98c50d-5cea-f8fa-e4f9-190bbb536434 ==> amazon-ebs: Authorizing access to port 22 from 0.0.0.0/0 in the temporary security group... ==> amazon-ebs: Launching a source AWS instance... ==> amazon-ebs: Adding tags to source instance amazon-ebs: Adding tag: \"Application\": \"base\" amazon-ebs: Adding tag: \"Name\": \"rhel-base-packer\" amazon-ebs: Instance ID: i-03777753ff9da5a90 ==> amazon-ebs: Waiting for instance (i-03777753ff9da5a90) to become ready... ==> amazon-ebs: Using ssh communicator to connect: 54.229.138.43 ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Stopping the source instance... amazon-ebs: Stopping instance, attempt 1 ==> amazon-ebs: Waiting for the instance to stop... ==> amazon-ebs: Creating unencrypted AMI RHEL-BASE-v-1553515786-AMI from instance i-03777753ff9da5a90 amazon-ebs: AMI: ami-09e36b5de41d52201 ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Modifying attributes on AMI (ami-09e36b5de41d52201)... amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on snapshot (snap-03c17a67bf38f946f)... ==> amazon-ebs: Adding tags to AMI (ami-09e36b5de41d52201)... ==> amazon-ebs: Tagging snapshot: snap-03c17a67bf38f946f ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"Version\": \"\" amazon-ebs: Adding tag: \"Application\": \"Base Image\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"OS_Version\": \"RedHat7\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... ==> amazon-ebs: Running post-processor: vagrant ==> amazon-ebs (vagrant): Creating Vagrant box for 'aws' provider amazon-ebs (vagrant): Compressing: Vagrantfile amazon-ebs (vagrant): Compressing: metadata.json Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-09e36b5de41d52201 --> amazon-ebs: 'aws' provider box: packer_amazon-ebs_aws.box You get your regular AMI made plus you'll notice that the last few lines are different, instead of the usual build and output it leaves you you with a binary file packer_amazon-ebs_aws.box . This can be added to your Vagrant with: vagrant box add fancy-box packer_amazon-ebs_aws.box vagrant box add fancy -box packer_amazon-ebs_aws.box ==> box: Box file was not detected as metadata. Adding it directly... ==> box: Adding box 'fancy-box' (v0) for provider: box: Unpacking necessary files from: file://C:/code/book/Image-Creation-using-Packer/packer-ami/amazon-ebs/linux/packer_amazon-ebs_aws.box box: Progress: 100% (Rate: 349k/s, Estimated time remaining: --:--:--) ==> box: Successfully added box 'fancy-box' (v0) for 'aws'! Amazing!","title":"Post-processor"},{"location":"share/","text":"Sharing Sharing across regions You can use Packer and share your AMI across regions, Packer will copy your disk snapshot to the specified region[this will take a while depending on the disk volume size]. Code: https://github.com/JamesWoolfenden/learn-packer/tree/master/examples/share-to-region In amazon-ebs.base1604.pkr.hcl that's adding \"ami_regions = var.ami_regions\" source \"amazon-ebs\" \"base1604\" { ami_description= \"ubuntu base 16.04\" ami_name = \"ubuntu-16.04-BASE-v1-{{timestamp}}-AMI\" ami_users = var.ami_users ami_regions = var.ami_regions ami_virtualization_type= \"hvm\" associate_public_ip_address= var.associate_public_ip_address instance_type = var.instance_type region= var.region run_tags { Name= \"ubuntu-base-packer\" Application= \"base\" OS= \"Ubuntu 16.04\" } spot_price= \"auto\" ssh_username= \"ubuntu\" subnet_id= var.subnet_id source_ami_filter { filters { virtualization-type= \"hvm\" name= \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\" root-device-type= \"ebs\" } most_recent= true owners= [\"099720109477\"] } temporary_key_pair_name= \"ubuntu-packer-{{timestamp}}\" vpc_id= var.vpc_id tags { OS_Version= \"Ubuntu 16.04\" Version= var.BUILD_NUMBER Application= \"Ubuntu Image\" Runner= \"EC2\" } } and support for the variable file variables.pkr.hcl and its' values in values.auto.pkrvars.hcl . $ packer build share-to-region/ ... Sharing across accounts This is similar to regions, add in adding: ami_users = var.ami_users to amazon-ebs.base1604.pkr.hcl , only this won't take as long as the sharing cross region as your are only sharing access to resources, from the other accounts. Its fairly straight-forward for unencrypted AMIS. How to encrypt boot volume of image and share across accounts How to encrypt boot volume of image Add the details of your key you want to create and implement a template using https://registry.terraform.io/modules/JamesWoolfenden/kms/aws/0.0.3 . Create the customer KMS key and give it an alias, I used alias/ami-sharing In your encrypt\\amazon-ebs.base1604.pkr.hcl add: encrypt_boot = \"true\" kms_key_id = \"alias/ami-sharing\" If you want to build to multiple regions you will need to specify a key for each region: encrypt_boot = \"true\" region_kms_key_ids { eu-west-1= \"alias/ami-sharing\" eu-west-2= \"alias/ami-sharing\" } With these items included, a build will create an encrypted AMI for the regions specified. Create and share a KMS key This will encrypt the volumes, and the other accounts will need to read it to use the shared AMIS. Create and share a KMS key using this Terraform module https://github.com/JamesWoolfenden/terraform-aws-kms : module \"kms\" { source = \"JamesWoolfenden/kms/aws\" version = \"0.0.3\" common_tags = var.common_tags key = var.key accounts = var.accounts } But add to the list accounts specify the accounts you want to share to. See the folder examples/kms for a fully worked up example. The module creates a KMS key and shares it between any number of accounts. You will need a different key for each region, although the alias can be the same. $ packer build encrypt/ amazon-ebs: output will be in this color. ==> amazon-ebs: Prevalidating any provided VPC information ==> amazon-ebs: Prevalidating AMI Name: ubuntu-16.04-BASE-v1-1583705264-AMI amazon-ebs: Found Image ID: ami-0a590332f9f499197 ==> amazon-ebs: Creating temporary keypair: ubuntu-packer-1583705264 ==> amazon-ebs: Creating temporary security group for this instance: packer_5e656cb2-f6b3-eb4f-1e0f-b624af166369 ==> amazon-ebs: Authorizing access to port 22 from [0.0.0.0/0] in the temporary security groups... ==> amazon-ebs: Launching a spot AWS instance... ==> amazon-ebs: Interpolating tags for spot instance... amazon-ebs: Adding tag: \"OS\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Application\": \"base\" amazon-ebs: Adding tag: \"Name\": \"ubuntu-base-packer\" amazon-ebs: Loading User Data File... amazon-ebs: Creating Spot Fleet launch template... amazon-ebs: Sending spot request ()... amazon-ebs: Instance ID: i-01478606acfeed1e7 ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Creating AMI K08fNTm from instance i-01478606acfeed1e7 amazon-ebs: AMI: ami-0c4cbec3daaf0bac9 ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Copying/Encrypting AMI (ami-0c4cbec3daaf0bac9) to other regions... amazon-ebs: Copying to: eu-west-1 amazon-ebs: Copying to: eu-west-2 amazon-ebs: Waiting for all copies to complete... ==> amazon-ebs: Modifying attributes on AMI (ami-05f8bfd2d13e8257b)... amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on AMI (ami-07157019afe1400c8)... amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on snapshot (snap-0da1e3fe97a35bb60)... ==> amazon-ebs: Modifying attributes on snapshot (snap-054a565a034ef1102)... ==> amazon-ebs: Adding tags to AMI (ami-05f8bfd2d13e8257b)... ==> amazon-ebs: Tagging snapshot: snap-0da1e3fe97a35bb60 ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"Version\": \"1\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Adding tags to AMI (ami-07157019afe1400c8)... ==> amazon-ebs: Tagging snapshot: snap-054a565a034ef1102 ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"Version\": \"1\" amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Deregistering the AMI and deleting unencrypted temporary AMIs and snapshots ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-07157019afe1400c8 eu-west-2: ami-05f8bfd2d13e8257b Encrypt and share across accounts Update your kms key template account variables and apply, to share the key to the third party accounts. With the keys updated, and also the value of the list aws-regions with your new target AWS accounts rebuild the packer folder and you will have shared your encrypted ami cross region and cross account. packer build .\\encrypt\\ amazon-ebs: output will be in this color. ==> amazon-ebs: Prevalidating any provided VPC information ==> amazon-ebs: Prevalidating AMI Name: ubuntu-16.04-BASE-v1-1583750275-AMI amazon-ebs: Found Image ID: ami-0a590332f9f499197 ==> amazon-ebs: Creating temporary keypair: ubuntu-packer-1583750275 ==> amazon-ebs: Creating temporary security group for this instance: packer_5e661c84-fbe2-89df-ad10-e5cad9a12b7d ==> amazon-ebs: Authorizing access to port 22 from [0.0.0.0/0] in the temporary security groups... ==> amazon-ebs: Launching a spot AWS instance... ==> amazon-ebs: Interpolating tags for spot instance... amazon-ebs: Adding tag: \"Application\": \"base\" amazon-ebs: Adding tag: \"Name\": \"ubuntu-base-packer\" amazon-ebs: Adding tag: \"OS\": \"Ubuntu 16.04\" amazon-ebs: Loading User Data File... amazon-ebs: Creating Spot Fleet launch template... amazon-ebs: Sending spot request ()... amazon-ebs: Instance ID: i-0457e2990bd585d2d ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Creating AMI zOLxxQ9 from instance i-0457e2990bd585d2d amazon-ebs: AMI: ami-0bdae0ecb3c5268e4 ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Copying/Encrypting AMI (ami-0bdae0ecb3c5268e4) to other regions... amazon-ebs: Copying to: eu-west-2 amazon-ebs: Copying to: eu-west-1 amazon-ebs: Waiting for all copies to complete... ==> amazon-ebs: Modifying attributes on AMI (ami-0a6366419738405d6)... amazon-ebs: Modifying: description amazon-ebs: Modifying: users ==> amazon-ebs: Modifying attributes on AMI (ami-02e9526bdb3fe0179)... amazon-ebs: Modifying: users amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on snapshot (snap-08419a36afff3d151)... ==> amazon-ebs: Modifying attributes on snapshot (snap-05233b62e73920cbd)... ==> amazon-ebs: Adding tags to AMI (ami-0a6366419738405d6)... ==> amazon-ebs: Tagging snapshot: snap-08419a36afff3d151 ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"Version\": \"1\" amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Adding tags to AMI (ami-02e9526bdb3fe0179)... ==> amazon-ebs: Tagging snapshot: snap-05233b62e73920cbd ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"Version\": \"1\" amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Deregistering the AMI and deleting unencrypted temporary AMIs and snapshots ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-02e9526bdb3fe0179 eu-west-2: ami-0a6366419738405d6","title":"Sharing"},{"location":"share/#sharing","text":"","title":"Sharing"},{"location":"share/#sharing-across-regions","text":"You can use Packer and share your AMI across regions, Packer will copy your disk snapshot to the specified region[this will take a while depending on the disk volume size]. Code: https://github.com/JamesWoolfenden/learn-packer/tree/master/examples/share-to-region In amazon-ebs.base1604.pkr.hcl that's adding \"ami_regions = var.ami_regions\" source \"amazon-ebs\" \"base1604\" { ami_description= \"ubuntu base 16.04\" ami_name = \"ubuntu-16.04-BASE-v1-{{timestamp}}-AMI\" ami_users = var.ami_users ami_regions = var.ami_regions ami_virtualization_type= \"hvm\" associate_public_ip_address= var.associate_public_ip_address instance_type = var.instance_type region= var.region run_tags { Name= \"ubuntu-base-packer\" Application= \"base\" OS= \"Ubuntu 16.04\" } spot_price= \"auto\" ssh_username= \"ubuntu\" subnet_id= var.subnet_id source_ami_filter { filters { virtualization-type= \"hvm\" name= \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\" root-device-type= \"ebs\" } most_recent= true owners= [\"099720109477\"] } temporary_key_pair_name= \"ubuntu-packer-{{timestamp}}\" vpc_id= var.vpc_id tags { OS_Version= \"Ubuntu 16.04\" Version= var.BUILD_NUMBER Application= \"Ubuntu Image\" Runner= \"EC2\" } } and support for the variable file variables.pkr.hcl and its' values in values.auto.pkrvars.hcl . $ packer build share-to-region/ ...","title":"Sharing across regions"},{"location":"share/#sharing-across-accounts","text":"This is similar to regions, add in adding: ami_users = var.ami_users to amazon-ebs.base1604.pkr.hcl , only this won't take as long as the sharing cross region as your are only sharing access to resources, from the other accounts. Its fairly straight-forward for unencrypted AMIS.","title":"Sharing across accounts"},{"location":"share/#how-to-encrypt-boot-volume-of-image-and-share-across-accounts","text":"","title":"How to encrypt boot volume of image and share across accounts"},{"location":"share/#how-to-encrypt-boot-volume-of-image","text":"Add the details of your key you want to create and implement a template using https://registry.terraform.io/modules/JamesWoolfenden/kms/aws/0.0.3 . Create the customer KMS key and give it an alias, I used alias/ami-sharing In your encrypt\\amazon-ebs.base1604.pkr.hcl add: encrypt_boot = \"true\" kms_key_id = \"alias/ami-sharing\" If you want to build to multiple regions you will need to specify a key for each region: encrypt_boot = \"true\" region_kms_key_ids { eu-west-1= \"alias/ami-sharing\" eu-west-2= \"alias/ami-sharing\" } With these items included, a build will create an encrypted AMI for the regions specified.","title":"How to encrypt boot volume of image"},{"location":"share/#create-and-share-a-kms-key","text":"This will encrypt the volumes, and the other accounts will need to read it to use the shared AMIS. Create and share a KMS key using this Terraform module https://github.com/JamesWoolfenden/terraform-aws-kms : module \"kms\" { source = \"JamesWoolfenden/kms/aws\" version = \"0.0.3\" common_tags = var.common_tags key = var.key accounts = var.accounts } But add to the list accounts specify the accounts you want to share to. See the folder examples/kms for a fully worked up example. The module creates a KMS key and shares it between any number of accounts. You will need a different key for each region, although the alias can be the same. $ packer build encrypt/ amazon-ebs: output will be in this color. ==> amazon-ebs: Prevalidating any provided VPC information ==> amazon-ebs: Prevalidating AMI Name: ubuntu-16.04-BASE-v1-1583705264-AMI amazon-ebs: Found Image ID: ami-0a590332f9f499197 ==> amazon-ebs: Creating temporary keypair: ubuntu-packer-1583705264 ==> amazon-ebs: Creating temporary security group for this instance: packer_5e656cb2-f6b3-eb4f-1e0f-b624af166369 ==> amazon-ebs: Authorizing access to port 22 from [0.0.0.0/0] in the temporary security groups... ==> amazon-ebs: Launching a spot AWS instance... ==> amazon-ebs: Interpolating tags for spot instance... amazon-ebs: Adding tag: \"OS\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Application\": \"base\" amazon-ebs: Adding tag: \"Name\": \"ubuntu-base-packer\" amazon-ebs: Loading User Data File... amazon-ebs: Creating Spot Fleet launch template... amazon-ebs: Sending spot request ()... amazon-ebs: Instance ID: i-01478606acfeed1e7 ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Creating AMI K08fNTm from instance i-01478606acfeed1e7 amazon-ebs: AMI: ami-0c4cbec3daaf0bac9 ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Copying/Encrypting AMI (ami-0c4cbec3daaf0bac9) to other regions... amazon-ebs: Copying to: eu-west-1 amazon-ebs: Copying to: eu-west-2 amazon-ebs: Waiting for all copies to complete... ==> amazon-ebs: Modifying attributes on AMI (ami-05f8bfd2d13e8257b)... amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on AMI (ami-07157019afe1400c8)... amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on snapshot (snap-0da1e3fe97a35bb60)... ==> amazon-ebs: Modifying attributes on snapshot (snap-054a565a034ef1102)... ==> amazon-ebs: Adding tags to AMI (ami-05f8bfd2d13e8257b)... ==> amazon-ebs: Tagging snapshot: snap-0da1e3fe97a35bb60 ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"Version\": \"1\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Adding tags to AMI (ami-07157019afe1400c8)... ==> amazon-ebs: Tagging snapshot: snap-054a565a034ef1102 ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"Version\": \"1\" amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Deregistering the AMI and deleting unencrypted temporary AMIs and snapshots ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-07157019afe1400c8 eu-west-2: ami-05f8bfd2d13e8257b","title":"Create and share a KMS key"},{"location":"share/#encrypt-and-share-across-accounts","text":"Update your kms key template account variables and apply, to share the key to the third party accounts. With the keys updated, and also the value of the list aws-regions with your new target AWS accounts rebuild the packer folder and you will have shared your encrypted ami cross region and cross account. packer build .\\encrypt\\ amazon-ebs: output will be in this color. ==> amazon-ebs: Prevalidating any provided VPC information ==> amazon-ebs: Prevalidating AMI Name: ubuntu-16.04-BASE-v1-1583750275-AMI amazon-ebs: Found Image ID: ami-0a590332f9f499197 ==> amazon-ebs: Creating temporary keypair: ubuntu-packer-1583750275 ==> amazon-ebs: Creating temporary security group for this instance: packer_5e661c84-fbe2-89df-ad10-e5cad9a12b7d ==> amazon-ebs: Authorizing access to port 22 from [0.0.0.0/0] in the temporary security groups... ==> amazon-ebs: Launching a spot AWS instance... ==> amazon-ebs: Interpolating tags for spot instance... amazon-ebs: Adding tag: \"Application\": \"base\" amazon-ebs: Adding tag: \"Name\": \"ubuntu-base-packer\" amazon-ebs: Adding tag: \"OS\": \"Ubuntu 16.04\" amazon-ebs: Loading User Data File... amazon-ebs: Creating Spot Fleet launch template... amazon-ebs: Sending spot request ()... amazon-ebs: Instance ID: i-0457e2990bd585d2d ==> amazon-ebs: Waiting for SSH to become available... ==> amazon-ebs: Connected to SSH! ==> amazon-ebs: Creating AMI zOLxxQ9 from instance i-0457e2990bd585d2d amazon-ebs: AMI: ami-0bdae0ecb3c5268e4 ==> amazon-ebs: Waiting for AMI to become ready... ==> amazon-ebs: Copying/Encrypting AMI (ami-0bdae0ecb3c5268e4) to other regions... amazon-ebs: Copying to: eu-west-2 amazon-ebs: Copying to: eu-west-1 amazon-ebs: Waiting for all copies to complete... ==> amazon-ebs: Modifying attributes on AMI (ami-0a6366419738405d6)... amazon-ebs: Modifying: description amazon-ebs: Modifying: users ==> amazon-ebs: Modifying attributes on AMI (ami-02e9526bdb3fe0179)... amazon-ebs: Modifying: users amazon-ebs: Modifying: description ==> amazon-ebs: Modifying attributes on snapshot (snap-08419a36afff3d151)... ==> amazon-ebs: Modifying attributes on snapshot (snap-05233b62e73920cbd)... ==> amazon-ebs: Adding tags to AMI (ami-0a6366419738405d6)... ==> amazon-ebs: Tagging snapshot: snap-08419a36afff3d151 ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" amazon-ebs: Adding tag: \"Version\": \"1\" amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Adding tags to AMI (ami-02e9526bdb3fe0179)... ==> amazon-ebs: Tagging snapshot: snap-05233b62e73920cbd ==> amazon-ebs: Creating AMI tags amazon-ebs: Adding tag: \"Version\": \"1\" amazon-ebs: Adding tag: \"Application\": \"Ubuntu Image\" amazon-ebs: Adding tag: \"OS_Version\": \"Ubuntu 16.04\" amazon-ebs: Adding tag: \"Runner\": \"EC2\" ==> amazon-ebs: Creating snapshot tags ==> amazon-ebs: Deregistering the AMI and deleting unencrypted temporary AMIs and snapshots ==> amazon-ebs: Terminating the source AWS instance... ==> amazon-ebs: Cleaning up any extra volumes... ==> amazon-ebs: No volumes to clean up, skipping ==> amazon-ebs: Deleting temporary security group... ==> amazon-ebs: Deleting temporary keypair... Build 'amazon-ebs' finished. ==> Builds finished. The artifacts of successful builds are: --> amazon-ebs: AMIs were created: eu-west-1: ami-02e9526bdb3fe0179 eu-west-2: ami-0a6366419738405d6","title":"Encrypt and share across accounts"},{"location":"variables/","text":"Variables There are multiple ways use variables with Packer Templates. From cmd line packer build base.json -var 'region=eu-west-1' From a file packer build base.json -var-file env.json Where the env.json looks like: { \"instance_type\" : \"t2.micro\" , \"vpc_id\" : \"vpc-xxxxxxx\" , \"subnet_id\" : \"subnet-xxxxxxx\" , \"ami_users\" : \"xxxxxxxxxx\" , \"aws_region\" : \"eu-west-1\" } From Environmental Variables Any environmental Variable can be used in the template, to get AWS_ACCESS_KEY_ID: { {env `AWS_ACCESS_KEY_ID` } } From Consul You can access key/value in consul: { { consul_key `my/lovely/horse` } } From Vault And Secret or sensitive variables from Hashicorp Vault: { { vault `/my/secret/lovely` `horse` } }","title":"Variables"},{"location":"variables/#variables","text":"There are multiple ways use variables with Packer Templates.","title":"Variables"},{"location":"variables/#from-cmd-line","text":"packer build base.json -var 'region=eu-west-1'","title":"From cmd line"},{"location":"variables/#from-a-file","text":"packer build base.json -var-file env.json Where the env.json looks like: { \"instance_type\" : \"t2.micro\" , \"vpc_id\" : \"vpc-xxxxxxx\" , \"subnet_id\" : \"subnet-xxxxxxx\" , \"ami_users\" : \"xxxxxxxxxx\" , \"aws_region\" : \"eu-west-1\" }","title":"From a file"},{"location":"variables/#from-environmental-variables","text":"Any environmental Variable can be used in the template, to get AWS_ACCESS_KEY_ID: { {env `AWS_ACCESS_KEY_ID` } }","title":"From Environmental Variables"},{"location":"variables/#from-consul","text":"You can access key/value in consul: { { consul_key `my/lovely/horse` } }","title":"From Consul"},{"location":"variables/#from-vault","text":"And Secret or sensitive variables from Hashicorp Vault: { { vault `/my/secret/lovely` `horse` } }","title":"From Vault"}]}